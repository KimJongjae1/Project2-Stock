#!/usr/bin/env python3
"""
ë‹¨ìˆœí™”ëœ KOSPI 200 ê¸°ì—… ëŒ€ìƒ Docker Spark í´ëŸ¬ìŠ¤í„°ìš© PageRank ë¶„ì„ê¸°
- ì§ë ¬í™” ë¬¸ì œ ì™„ì „ í•´ê²°
- ì‚¬ìš©ë²•: docker exec -it spark-client python /opt/spark/jobs/spark_pageRank_kospi200_simple.py
"""

from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql.types import *
from pyspark.sql.window import Window
import pandas as pd
import numpy as np
import networkx as nx
import os
import glob
import sys
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ê¸€ë¡œë²Œ KOSPI 200 ê¸°ì—… ë¦¬ìŠ¤íŠ¸ (í´ë˜ìŠ¤ ì™¸ë¶€ì— ì •ì˜)
KOSPI200_COMPANIES = [
    "BGFë¦¬í…Œì¼","BNKê¸ˆìœµì§€ì£¼","CJ","CJëŒ€í•œí†µìš´","CJì œì¼ì œë‹¹",
    "DBì†í•´ë³´í—˜","DL","DLì´ì•¤ì”¨","DNì˜¤í† ëª¨í‹°ë¸Œ","F&F",
    "GKL","GS","GSê±´ì„¤","GSë¦¬í…Œì¼","HDC",
    "HDí•œêµ­ì¡°ì„ í•´ì–‘","HDí˜„ëŒ€","HDí˜„ëŒ€ë§ˆë¦°ì†”ë£¨ì…˜","HDí˜„ëŒ€ë¯¸í¬","HDí˜„ëŒ€ì¸í”„ë¼ì½”ì–´",
    "HDí˜„ëŒ€ì¼ë ‰íŠ¸ë¦­","HDí˜„ëŒ€ì¤‘ê³µì—…","HLë§Œë„","HMM","HSíš¨ì„±ì²¨ë‹¨ì†Œì¬",
    "JBê¸ˆìœµì§€ì£¼","KBê¸ˆìœµ","KCC","KGëª¨ë¹Œë¦¬í‹°","KT",
    "KT&G",
    "LG","LGë””ìŠ¤í”Œë ˆì´","LGìƒí™œê±´ê°•","LGì—ë„ˆì§€ì†”ë£¨ì…˜","LGìœ í”ŒëŸ¬ìŠ¤",
    "LGì´ë…¸í…","LGì „ì","LGí™”í•™","LIGë„¥ìŠ¤ì›","LS",
    "LS ELECTRIC","NAVER","NHíˆ¬ìì¦ê¶Œ","OCI","OCIí™€ë”©ìŠ¤",
    "POSCOí™€ë”©ìŠ¤","S-Oil","SK","SKC","SKë°”ì´ì˜¤ì‚¬ì´ì–¸ìŠ¤",
    "SKë°”ì´ì˜¤íŒœ","SKìŠ¤í€˜ì–´","SKì•„ì´ì´í…Œí¬ë†€ë¡œì§€","SKì´ë…¸ë² ì´ì…˜","SKì¼€ë¯¸ì¹¼",
    "SKí…”ë ˆì½¤","SKí•˜ì´ë‹‰ìŠ¤","TCCìŠ¤í‹¸","TKGíœ´ì¼ìŠ¤","iMê¸ˆìœµì§€ì£¼",
    "ê°•ì›ëœë“œ","ê³ ë ¤ì•„ì—°","ê¸ˆí˜¸ì„ìœ í™”í•™","ê¸ˆí˜¸íƒ€ì´ì–´","ê¸°ì•„",
    "ê¸°ì—…ì€í–‰","ë„·ë§ˆë¸”","ë…¹ì‹­ì","ë…¹ì‹­ìí™€ë”©ìŠ¤","ë†ì‹¬",
    "ëŒ€ìƒ","ëŒ€ìš°ê±´ì„¤","ëŒ€ì›…","ëŒ€ì›…ì œì•½","ëŒ€í•œìœ í™”",
    "ëŒ€í•œì „ì„ ","ëŒ€í•œí•­ê³µ","ë”ë¸”ìœ ê²Œì„ì¦ˆ","ë´í‹°ì›€","ë™ì„œ",
    "ë™ì›ì‚°ì—…","ë™ì›ì‹œìŠ¤í…œì¦ˆ","ë‘ì‚°","ë‘ì‚°ë¡œë³´í‹±ìŠ¤","ë‘ì‚°ë°¥ìº£",
    "ë‘ì‚°ì—ë„ˆë¹Œë¦¬í‹°","ë¡¯ë°ì‡¼í•‘","ë¡¯ë°ì›°í‘¸ë“œ","ë¡¯ë°ì •ë°€í™”í•™","ë¡¯ë°ì§€ì£¼",
    "ë¡¯ë°ì¹ ì„±","ë¡¯ë°ì¼€ë¯¸ì¹¼","ë©”ë¦¬ì¸ ê¸ˆìœµì§€ì£¼","ë¯¸ë˜ì—ì…‹ì¦ê¶Œ","ë¯¸ìŠ¤í† í™€ë”©ìŠ¤",
    "ë¯¸ì›ìƒì‚¬","ë¯¸ì›ì—ìŠ¤ì”¨","ì‚¼ì„±E&A","ì‚¼ì„±SDI","ì‚¼ì„±ë¬¼ì‚°",
    "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤","ì‚¼ì„±ìƒëª…","ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤","ì‚¼ì„±ì „ê¸°","ì‚¼ì„±ì „ì",
    "ì‚¼ì„±ì¤‘ê³µì—…","ì‚¼ì„±ì¦ê¶Œ","ì‚¼ì„±ì¹´ë“œ","ì‚¼ì„±í™”ì¬","ì‚¼ì–‘ì‹í’ˆ",
    "ì„¸ë°©ì „ì§€","ì„¸ì•„ë² ìŠ¤í‹¸ì§€ì£¼","ì„¸ì•„ì œê°•ì§€ì£¼","ì…€íŠ¸ë¦¬ì˜¨","ì‹ ì„¸ê³„",
    "ì‹ í•œì§€ì£¼","ì”¨ì—ìŠ¤ìœˆë“œ","ì•„ëª¨ë ˆí¼ì‹œí”½","ì•„ëª¨ë ˆí¼ì‹œí”½í™€ë”©ìŠ¤","ì—ìŠ¤ë””ë°”ì´ì˜¤ì„¼ì„œ",
    "ì—ìŠ¤ì—˜","ì—ìŠ¤ì›","ì—ì´í”¼ì•Œ","ì—ì½”í”„ë¡œë¨¸í‹°","ì—”ì”¨ì†Œí”„íŠ¸",
    "ì—˜ì•¤ì—í”„","ì˜ì›ë¬´ì—­","ì˜ì›ë¬´ì—­í™€ë”©ìŠ¤","ì˜í’","ì˜¤ëšœê¸°",
    "ì˜¤ë¦¬ì˜¨","ì˜¤ë¦¬ì˜¨í™€ë”©ìŠ¤","ìš°ë¦¬ê¸ˆìœµì§€ì£¼","ìœ í•œì–‘í–‰","ìœ¨ì´Œí™”í•™",
    "ì´ë§ˆíŠ¸","ì´ìˆ˜ìŠ¤í˜ì…œí‹°ì¼€ë¯¸ì»¬","ì œì¼ê¸°íš","ì¢…ê·¼ë‹¹","ì§€ì—­ë‚œë°©ê³µì‚¬",
    "ì¹´ì¹´ì˜¤","ì¹´ì¹´ì˜¤ë±…í¬","ì¹´ì¹´ì˜¤í˜ì´","ì½”ìŠ¤ë§¥ìŠ¤","ì½”ìŠ¤ëª¨í™”í•™",
    "ì½”ì˜¤ë¡±ì¸ë”","ì½”ì›¨ì´","í¬ë˜í”„í†¤","í‚¤ì›€ì¦ê¶Œ","íƒœê´‘ì‚°ì—…",
    "íŒ¬ì˜¤ì…˜","í¬ìŠ¤ì½”DX","í¬ìŠ¤ì½”ì¸í„°ë‚´ì…”ë„","í¬ìŠ¤ì½”í“¨ì²˜ì— ","í’ì‚°",
    "í•˜ë‚˜ê¸ˆìœµì§€ì£¼","í•˜ë‚˜íˆ¬ì–´","í•˜ì´ë¸Œ","í•˜ì´íŠ¸ì§„ë¡œ","í•œêµ­ê°€ìŠ¤ê³µì‚¬",
    "í•œêµ­ê¸ˆìœµì§€ì£¼","í•œêµ­ì•¤ì»´í¼ë‹ˆ","í•œêµ­ì „ë ¥","í•œêµ­ì¹´ë³¸","í•œêµ­ì½œë§ˆ",
    "í•œêµ­íƒ€ì´ì–´ì•¤í…Œí¬ë†€ë¡œì§€","í•œêµ­í•­ê³µìš°ì£¼","í•œë¯¸ë°˜ë„ì²´","í•œë¯¸ì‚¬ì´ì–¸ìŠ¤","í•œë¯¸ì•½í’ˆ",
    "í•œìƒ˜","í•œì†”ì¼€ë¯¸ì¹¼","í•œì˜¨ì‹œìŠ¤í…œ","í•œì˜¬ë°”ì´ì˜¤íŒŒë§ˆ","í•œì¼ì‹œë©˜íŠ¸",
    "í•œì „KPS","í•œì „ê¸°ìˆ ","í•œì§„ì¹¼","í•œí™”","í•œí™”ë¹„ì „",
    "í•œí™”ìƒëª…","í•œí™”ì†”ë£¨ì…˜","í•œí™”ì‹œìŠ¤í…œ","í•œí™”ì—ì–´ë¡œìŠ¤í˜ì´ìŠ¤","í•œí™”ì˜¤ì…˜",
    "í˜„ëŒ€ê±´ì„¤","í˜„ëŒ€ê¸€ë¡œë¹„ìŠ¤","í˜„ëŒ€ë¡œí…œ","í˜„ëŒ€ëª¨ë¹„ìŠ¤","í˜„ëŒ€ë°±í™”ì ",
    "í˜„ëŒ€ì—˜ë¦¬ë² ì´í„°","í˜„ëŒ€ìœ„ì•„","í˜„ëŒ€ì œì² ","í˜„ëŒ€ì°¨","í˜„ëŒ€í•´ìƒ",
    "í˜¸í…”ì‹ ë¼","íš¨ì„±ì¤‘ê³µì—…","íš¨ì„±í‹°ì•¤ì”¨","í›„ì„±"
]

def init_spark_session():
    """Spark í´ëŸ¬ìŠ¤í„° ì„¸ì…˜ ì´ˆê¸°í™” (Docker/EC2 ìë™ ê°ì§€)"""
    try:
        # ì‹¤í–‰ í™˜ê²½ ê°ì§€
        is_docker = os.path.exists('/opt/spark/data')
        is_ec2 = os.path.exists('/opt/spark')
        
        if is_docker:
            print("ğŸ³ Docker Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì¤‘...")
            master_url = "spark://spark-master:7077"
        elif is_ec2:
            print("â˜ï¸ EC2 Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì¤‘...")
            master_url = "spark://localhost:7077"
        else:
            print("ğŸ’» ë¡œì»¬ Spark ì„¸ì…˜ ì‹œì‘...")
            master_url = "local[*]"
        
        spark = SparkSession.builder \
            .appName("SimpleKOSPI200PageRank") \
            .master(master_url) \
            .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
            .config("spark.sql.shuffle.partitions", "400") \
            .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
            .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
            .config("spark.executor.memory", "3g") \
            .config("spark.executor.memoryOverhead", "1g") \
            .config("spark.driver.memory", "2g") \
            .config("spark.network.timeout", "800s") \
            .config("spark.sql.broadcastTimeout", "600") \
            .getOrCreate()
        
        print(f"âœ… Docker Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì™„ë£Œ!")
        print(f"   Spark ë²„ì „: {spark.version}")
        
        # S3 ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜)
        try:
            hconf = spark._jsc.hadoopConfiguration()
            hconf.set("fs.s3a.aws.credentials.provider", "com.amazonaws.auth.EnvironmentVariableCredentialsProvider")
            region = os.getenv("AWS_DEFAULT_REGION") or os.getenv("AWS_REGION")
            if region:
                hconf.set("fs.s3a.endpoint", f"s3.{region}.amazonaws.com")
            if os.getenv("S3_PATH_STYLE", "false").lower() in ("1","true","yes"):
                hconf.set("fs.s3a.path.style.access", "true")
        except Exception:
            pass
        
        return spark
        
    except Exception as e:
        print(f"âŒ Docker Spark í´ëŸ¬ìŠ¤í„° ì—°ê²° ì‹¤íŒ¨: {e}")
        sys.exit(1)

def load_data(spark, file_path):
    """CSV ë˜ëŠ” Excel íŒŒì¼ì„ Spark DataFrameìœ¼ë¡œ ë¡œë“œ (ë¡œì»¬/S3 ì§€ì›)"""
    
    print("ğŸ“ íŒŒì¼ ë¡œë“œ ì¤‘...")
    print("=" * 50)
    
    try:
        # S3 ê²½ë¡œ í™•ì¸
        is_s3_path = file_path.startswith('s3a://') or file_path.startswith('s3://')
        
        if is_s3_path:
            print(f"â˜ï¸ S3 íŒŒì¼ ê°ì§€: {file_path}")
        else:
            print(f"ğŸ“„ ë¡œì»¬ íŒŒì¼ ê°ì§€: {file_path}")
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸
        if file_path.endswith('.xlsx') or file_path.endswith('.xls'):
            print("ğŸ“Š Excel íŒŒì¼ ê°ì§€ - Pandasë¡œ ì½ê¸°")
            
            if is_s3_path:
                print("âš ï¸ S3ì˜ Excel íŒŒì¼ì€ ì§ì ‘ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CSVë¡œ ë³€í™˜ í›„ ì‚¬ìš©í•˜ì„¸ìš”.")
                return None
            
            # Pandasë¡œ ì—‘ì…€ ì½ê¸°
            import pandas as pd
            pandas_df = pd.read_excel(file_path)
            
            # Spark DataFrameìœ¼ë¡œ ë³€í™˜
            news_df = spark.createDataFrame(pandas_df)
            
            print(f"âœ… Excel íŒŒì¼ ë¡œë“œ ì„±ê³µ!")
            print(f"   ì›ë³¸ í–‰ ìˆ˜: {len(pandas_df):,}ê±´")
            
        else:
            print("ğŸ“„ CSV íŒŒì¼ ê°ì§€ - Sparkë¡œ ì½ê¸°")
            
            # CSV íŒŒì¼ ì½ê¸° (ë‹¤ì–‘í•œ ì˜µì…˜ ì‹œë„)
            news_df = spark.read \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .option("encoding", "UTF-8") \
                .option("sep", ",") \
                .option("quote", '"') \
                .option("escape", '"') \
                .option("multiLine", "true") \
                .option("ignoreLeadingWhiteSpace", "true") \
                .option("ignoreTrailingWhiteSpace", "true") \
                .csv(file_path)
        
        news_df.cache()
        
        total_count = news_df.count()
        column_count = len(news_df.columns)
        
        print(f"âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ!")
        print(f"   ì „ì²´ ë‰´ìŠ¤: {total_count:,}ê±´")
        print(f"   ì»¬ëŸ¼ ìˆ˜: {column_count}ê°œ")
        
        # ì»¬ëŸ¼ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ“‹ ì»¬ëŸ¼ ì •ë³´:")
        print("-" * 60)
        for i, col in enumerate(news_df.columns, 1):
            print(f"   {i:2d}. {col}")
        
        # ìƒ˜í”Œ ë°ì´í„° í™•ì¸
        print("\nğŸ” ë°ì´í„° ìƒ˜í”Œ í™•ì¸:")
        print("-" * 80)
        sample_data = news_df.limit(3).collect()
        for i, row in enumerate(sample_data, 1):
            print(f"\nğŸ“„ ìƒ˜í”Œ {i}:")
            row_dict = dict(row.asDict())
            for key, value in row_dict.items():
                # ê°’ì´ ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ í‘œì‹œ
                if isinstance(value, str) and len(value) > 100:
                    display_value = value[:100] + "..."
                else:
                    display_value = value
                print(f"   {key}: {display_value}")
            print("-" * 40)
        
        return news_df
        
    except Exception as e:
        print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def extract_kospi200_connections(spark, news_df):
    """KOSPI 200 ê¸°ì—… ê°„ ì—°ê²° ê´€ê³„ ì¶”ì¶œ (ì™„ì „ í•¨ìˆ˜í˜•)"""
    
    print(f"\nğŸ”— KOSPI 200 ê¸°ì—… ê°„ ì—°ê²° ê´€ê³„ ì¶”ì¶œ")
    print("=" * 50)
    
    # 'ê¸°ê´€' ì»¬ëŸ¼ ì°¾ê¸°
    org_column = None
    columns = news_df.columns
    print(columns)
    for column_name in columns:
        if 'ê¸°ê´€' in column_name:
            org_column = column_name
            break
    
    if org_column is None:
        print("âŒ 'ê¸°ê´€' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
        return None
    
    print(f"âœ… ê¸°ê´€ ì»¬ëŸ¼ ë°œê²¬: '{org_column}'")
    
    try:
        # 1ë‹¨ê³„: ê¸°ë³¸ í•„í„°ë§
        print("ğŸ”„ 1ë‹¨ê³„: ê¸°ë³¸ ë°ì´í„° í•„í„°ë§...")
        valid_news = news_df.filter(
            F.col(org_column).isNotNull() & 
            (F.col(org_column) != "") &
            (F.length(F.col(org_column)) > 1)
        )
        
        valid_count = valid_news.count()
        print(f"   ìœ íš¨í•œ ë‰´ìŠ¤: {valid_count:,}ê±´")
        
        if valid_count == 0:
            print("âŒ ìœ íš¨í•œ ê¸°ê´€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤!")
            return None
        
        # 2ë‹¨ê³„: ë‰´ìŠ¤ë³„ ID ì¶”ê°€í•˜ì—¬ ê°™ì€ ë‰´ìŠ¤ ë‚´ ê¸°ì—…ë“¤ ê·¸ë£¹í•‘
        print("ğŸ”„ 2ë‹¨ê³„: ë‰´ìŠ¤ë³„ ID ì¶”ê°€...")
        news_with_id = valid_news.select(
            F.monotonically_increasing_id().alias("news_id"),
            F.col(org_column).alias("companies")
        )
        
        # 3ë‹¨ê³„: ê¸°ì—…ëª… ë¶„ë¦¬
        print("ğŸ”„ 3ë‹¨ê³„: ê¸°ì—…ëª… ë¶„ë¦¬...")
        companies_exploded = news_with_id.select(
            "news_id",
            F.explode(F.split(F.col("companies"), ",")).alias("company")
        ).select(
            "news_id",
            F.trim(F.col("company")).alias("company")
        ).filter(
            (F.col("company") != "") & 
            (F.length(F.col("company")) > 1)
        )
        
        # 4ë‹¨ê³„: KOSPI 200 ê¸°ì—…ë§Œ í•„í„°ë§ (IN ì—°ì‚°ì ì‚¬ìš©)
        print("ğŸ”„ 4ë‹¨ê³„: KOSPI 200 ê¸°ì—… í•„í„°ë§...")
        kospi200_companies_filtered = companies_exploded.filter(
            F.col("company").isin(KOSPI200_COMPANIES)
        )
        
        kospi_count = kospi200_companies_filtered.select("company").distinct().count()
        print(f"   ë°œê²¬ëœ KOSPI 200 ê¸°ì—…: {kospi_count}ê°œ")
        
        if kospi_count == 0:
            print("âŒ ë°ì´í„°ì—ì„œ KOSPI 200 ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return None
        
        # 5ë‹¨ê³„: ê°™ì€ ë‰´ìŠ¤ ë‚´ì—ì„œ ê¸°ì—… ê°„ ì—°ê²° ìƒì„± (ë‰´ìŠ¤ ë‚´ ë™ì¼ ê¸°ì—… ì¤‘ë³µ ì œê±° í›„ ìŒ ìƒì„±)
        print("ğŸ”„ 5ë‹¨ê³„: ê¸°ì—… ê°„ ì—°ê²° ê´€ê³„ ìƒì„±...")
        # ë‰´ìŠ¤ ë‚´ ì¤‘ë³µ ê¸°ì—… ì œê±°
        dedup_companies = kospi200_companies_filtered.select("news_id", "company").distinct()

        # Self-joinìœ¼ë¡œ ê°™ì€ ë‰´ìŠ¤ ë‚´ ê¸°ì—… ìŒ ìƒì„± (ì‚¬ì „ìˆœ ì •ë ¬ë¡œ ì¤‘ë³µ ì œê±°)
        connections = dedup_companies.alias("c1").join(
            dedup_companies.alias("c2"),
            (F.col("c1.news_id") == F.col("c2.news_id")) & 
            (F.col("c1.company") < F.col("c2.company"))  # ì¤‘ë³µ ì œê±° ë° ìˆœì„œ ì •ë ¬
        ).select(
            F.col("c1.company").alias("company1"),
            F.col("c2.company").alias("company2")
        )
        
        # 6ë‹¨ê³„: ì—°ê²° ê°•ë„ ê³„ì‚°
        print("ğŸ”„ 6ë‹¨ê³„: ì—°ê²° ê°•ë„ ê³„ì‚°...")
        connections_df = connections.groupBy("company1", "company2") \
            .count() \
            .withColumnRenamed("count", "weight") \
            .filter(F.col("weight") > 0)
        
        connections_df.cache()
        
        # í†µê³„
        connection_count = connections_df.count()
        
        if connection_count == 0:
            print("âŒ KOSPI 200 ê¸°ì—… ê°„ ì—°ê²° ê´€ê³„ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            return None
        
        avg_weight = connections_df.agg(F.avg("weight")).collect()[0][0]
        max_weight = connections_df.agg(F.max("weight")).collect()[0][0]
        
        participating_companies = connections_df.select("company1").union(
            connections_df.select("company2")
        ).distinct().count()
        
        print(f"ğŸ“Š KOSPI 200 ì¶”ì¶œ ê²°ê³¼:")
        print(f"   ì°¸ì—¬ ê¸°ì—… ìˆ˜: {participating_companies}ê°œ")
        print(f"   ì´ ì—°ê²° ê´€ê³„: {connection_count:,}ê°œ")
        print(f"   í‰ê·  ì—°ê²° ê°•ë„: {avg_weight:.1f}íšŒ")
        print(f"   ìµœëŒ€ ì—°ê²° ê°•ë„: {max_weight}íšŒ")
        
        # ìƒìœ„ ì—°ê²° ê´€ê³„ ì¶œë ¥
        print(f"\nğŸ”— ê°•í•œ ì—°ê²° ê´€ê³„ TOP 5:")
        top_connections = connections_df.orderBy(F.desc("weight")).limit(5).collect()
        for i, row in enumerate(top_connections, 1):
            print(f"   {i}. {row['company1']} â†” {row['company2']}: {row['weight']}íšŒ")
        
        return connections_df
        
    except Exception as e:
        print(f"âŒ ì—°ê²° ê´€ê³„ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def calculate_pagerank(spark, connections_df):
    """KOSPI 200 PageRank ê³„ì‚° (í•¨ìˆ˜í˜•)"""
    
    print(f"\nğŸ† KOSPI 200 PageRank ê³„ì‚°")
    print("=" * 40)
    
    try:
        # ì •ì  ì¤€ë¹„
        vertices = connections_df.select(F.col("company1").alias("company")).union(
            connections_df.select(F.col("company2").alias("company"))
        ).distinct()
        
        num_vertices = vertices.count()
        print(f"   ë¶„ì„ ëŒ€ìƒ ê¸°ì—… ìˆ˜: {num_vertices}ê°œ")
        
        edges = connections_df.select(
            F.col("company1").alias("src"),
            F.col("company2").alias("dst"),
            F.col("weight").cast("double")
        ).repartition(400, "src")

        # ë¬´ë°©í–¥ ë“±ê°€: ì—£ì§€ ëŒ€ì¹­í™” í›„ í•©ì¹˜ê¸° (ê°€ì¤‘ì¹˜ í•©ì‚°)
        edges_sym = edges.unionByName(
            edges.select(
                F.col("dst").alias("src"),
                F.col("src").alias("dst"),
                F.col("weight").alias("weight")
            )
        )
        edges = edges_sym.groupBy("src", "dst").agg(F.sum("weight").alias("weight"))
        
        # PageRank íŒŒë¼ë¯¸í„°
        damping = 0.85
        base_val = (1.0 - damping) / float(num_vertices)
        
        # ì´ˆê¸° rank
        ranks = vertices.withColumn("rank", F.lit(1.0 / float(num_vertices)))
        
        # out-degree ê³„ì‚°
        out_weight = edges.groupBy("src").agg(F.sum("weight").alias("out_w"))
        edges_norm = edges.join(out_weight, on="src", how="left").withColumn(
            "norm_w", F.when(F.col("out_w") > 0, F.col("weight") / F.col("out_w")).otherwise(F.lit(0.0))
        ).select("src", "dst", "norm_w")
        
        # ë°˜ë³µ ê³„ì‚° (ìˆ˜ë ´ ì¡°ê±´ ì¶”ê°€)
        iterations = 30  # 25 â†’ 30ìœ¼ë¡œ ì¦ê°€
        convergence_threshold = 1e-6  # ìˆ˜ë ´ ì„ê³„ê°’
        print(f"ğŸ”„ PageRank ë°˜ë³µ ê³„ì‚° (ìµœëŒ€ {iterations}íšŒ, ìˆ˜ë ´ ì„ê³„ê°’: {convergence_threshold})...")
        
        for i in range(iterations):
            # ì´ì „ rank ì €ì¥ (ìˆ˜ë ´ ì²´í¬ìš©)
            prev_ranks = ranks
            
            # contribution ê³„ì‚°
            contribs = edges_norm.join(
                ranks.withColumnRenamed("company", "src"), on="src", how="left"
            ).withColumn(
                "contrib", F.col("rank") * F.col("norm_w")
            ).groupBy("dst").agg(F.sum("contrib").alias("sum_contrib"))
            
            # ìƒˆë¡œìš´ rank ê³„ì‚°
            ranks = vertices.join(
                contribs.withColumnRenamed("dst", "company"), on="company", how="left"
            ).withColumn(
                "sum_contrib", F.coalesce(F.col("sum_contrib"), F.lit(0.0))
            ).withColumn(
                "rank", F.lit(base_val) + F.lit(damping) * F.col("sum_contrib")
            ).select("company", "rank")
            
            # ìˆ˜ë ´ ì²´í¬ (ë§¤ 5íšŒë§ˆë‹¤)
            if (i + 1) % 5 == 0:
                # rank ë³€í™”ëŸ‰ ê³„ì‚°
                rank_diff = ranks.join(
                    prev_ranks.withColumnRenamed("rank", "prev_rank"), 
                    on="company", how="inner"
                ).withColumn(
                    "diff", F.abs(F.col("rank") - F.col("prev_rank"))
                ).agg(F.max("diff")).collect()[0][0]
                
                print(f"   ë°˜ë³µ {i + 1}/{iterations} ì™„ë£Œ (ìµœëŒ€ ë³€í™”ëŸ‰: {rank_diff:.8f})")
                
                # ìˆ˜ë ´ í™•ì¸
                if rank_diff < convergence_threshold:
                    print(f"   âœ… {i + 1}íšŒ ë°˜ë³µì—ì„œ ìˆ˜ë ´ ì™„ë£Œ!")
                    break
        
        pagerank_results = ranks.select(
            F.col("company"), F.col("rank").alias("pagerank_score")
        ).orderBy(F.desc("pagerank_score"))
        
        print(f"âœ… PageRank ê³„ì‚° ì™„ë£Œ!")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ† KOSPI 200 ì˜í–¥ë ¥ ìˆœìœ„:")
        print(f"{'ìˆœìœ„':>4} {'ê¸°ì—…ëª…':>20} {'PageRank ì ìˆ˜':>15} {'ìƒëŒ€ ì ìˆ˜':>10}")
        print("-" * 60)
        
        top_results = pagerank_results.limit(15).collect()
        max_score = top_results[0]['pagerank_score'] if top_results else 0
        
        for i, row in enumerate(top_results, 1):
            company = row['company']
            score = row['pagerank_score']
            relative = (score / max_score) * 100 if max_score > 0 else 0
            print(f"{i:>4} {company:>20} {score:>15.6f} {relative:>9.1f}%")
        
        return pagerank_results
        
    except Exception as e:
        print(f"âŒ PageRank ê³„ì‚° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None

def analyze_company_from_s3(spark, company_name, s3_bucket, s3_prefix):
    """S3ì— ì €ì¥ëœ PageRank ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹ì • ê¸°ì—… ë¶„ì„"""
    
    print(f"\nğŸ” '{company_name}' ê¸°ì—… ìƒì„¸ ë¶„ì„ (S3 ê¸°ë°˜)")
    print("=" * 60)
    
    try:
        # S3ì—ì„œ PageRank ê²°ê³¼ ë¡œë“œ
        pagerank_path = f"s3a://{s3_bucket}/{s3_prefix}/pagerank/"
        connections_path = f"s3a://{s3_bucket}/{s3_prefix}/connections/"
        
        print(f"ğŸ“ S3ì—ì„œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        print(f"   PageRank: {pagerank_path}")
        print(f"   Connections: {connections_path}")
        
        # PageRank ê²°ê³¼ ë¡œë“œ
        try:
            pagerank_results = spark.read.parquet(pagerank_path)
            print(f"âœ… PageRank ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {pagerank_results.count()}ê°œ ê¸°ì—…")
        except Exception as e:
            print(f"âŒ PageRank ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ë¨¼ì € PageRank ë¶„ì„ì„ ì‹¤í–‰í•˜ì—¬ S3ì— ê²°ê³¼ë¥¼ ì €ì¥í•˜ì„¸ìš”.")
            return
        
        # ì—°ê²° ê´€ê³„ ë¡œë“œ
        try:
            connections_df = spark.read.parquet(connections_path)
            print(f"âœ… ì—°ê²° ê´€ê³„ ë¡œë“œ ì™„ë£Œ: {connections_df.count()}ê°œ ì—°ê²°")
        except Exception as e:
            print(f"âŒ ì—°ê²° ê´€ê³„ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì—°ê²° ê´€ê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            connections_df = None
        
        # 1. PageRank ì ìˆ˜ í™•ì¸
        company_pagerank = pagerank_results.filter(
            F.col("company").contains(company_name)
        ).collect()
        
        if not company_pagerank:
            print(f"âŒ '{company_name}' ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("ğŸ’¡ ê¸°ì—…ëª…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: ì‚¼ì„±ì „ì, í˜„ëŒ€ì°¨)")
            
            # ìœ ì‚¬í•œ ê¸°ì—…ëª… ì œì•ˆ
            print(f"\nğŸ” ìœ ì‚¬í•œ ê¸°ì—…ëª… ì œì•ˆ:")
            similar_companies = pagerank_results.filter(
                F.col("company").rlike(f".*{company_name}.*")
            ).limit(5).collect()
            
            if similar_companies:
                for i, row in enumerate(similar_companies, 1):
                    print(f"   {i}. {row['company']}")
            else:
                print("   ìœ ì‚¬í•œ ê¸°ì—…ëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 2. PageRank ì •ë³´
        company_score = company_pagerank[0]['pagerank_score']
        total_companies = pagerank_results.count()
        
        # ì „ì²´ ìˆœìœ„ ê³„ì‚°
        rank_position = pagerank_results.filter(
            F.col("pagerank_score") > company_score
        ).count() + 1
        
        print(f"ğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"   ê¸°ì—…ëª…: {company_pagerank[0]['company']}")
        print(f"   PageRank ì ìˆ˜: {company_score:.6f}")
        print(f"   ì „ì²´ ìˆœìœ„: {rank_position}ìœ„ / {total_companies}ê°œ ê¸°ì—…")
        print(f"   ìƒìœ„ {rank_position/total_companies*100:.1f}%")
        
        # 3. ì—°ê²° ê´€ê³„ ë¶„ì„ (ì—°ê²° ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
        if connections_df is not None:
            company_connections = connections_df.filter(
                (F.col("company1").contains(company_name)) | 
                (F.col("company2").contains(company_name))
            )
            
            total_connections = company_connections.count()
            
            if total_connections > 0:
                avg_weight = company_connections.agg(F.avg("weight")).collect()[0][0]
                max_weight = company_connections.agg(F.max("weight")).collect()[0][0]
                
                print(f"\nğŸ”— ì—°ê²° ê´€ê³„ ë¶„ì„:")
                print(f"   ì´ ì—°ê²° ìˆ˜: {total_connections}ê°œ")
                print(f"   í‰ê·  ì—°ê²° ê°•ë„: {avg_weight:.1f}íšŒ")
                print(f"   ìµœëŒ€ ì—°ê²° ê°•ë„: {max_weight}íšŒ")
                
                # 4. ì£¼ìš” ì—°ê²° ê¸°ì—…ë“¤
                print(f"\nğŸ¤ ì£¼ìš” ì—°ê²° ê¸°ì—… TOP 10:")
                print("-" * 50)
                
                # company1ì´ ëŒ€ìƒ ê¸°ì—…ì¸ ê²½ìš°
                connections_as_source = company_connections.filter(
                    F.col("company1").contains(company_name)
                ).select(
                    F.col("company2").alias("partner"),
                    F.col("weight")
                )
                
                # company2ê°€ ëŒ€ìƒ ê¸°ì—…ì¸ ê²½ìš°
                connections_as_target = company_connections.filter(
                    F.col("company2").contains(company_name)
                ).select(
                    F.col("company1").alias("partner"),
                    F.col("weight")
                )
                
                # ëª¨ë“  ì—°ê²° í†µí•©
                all_connections = connections_as_source.union(connections_as_target)
                
                # íŒŒíŠ¸ë„ˆë³„ ì´ ì—°ê²° ê°•ë„ ê³„ì‚°
                partner_connections = all_connections.groupBy("partner").agg(
                    F.sum("weight").alias("total_weight")
                ).orderBy(F.desc("total_weight")).limit(10)
                
                top_partners = partner_connections.collect()
                for i, row in enumerate(top_partners, 1):
                    print(f"   {i:2d}. {row['partner']}: {row['total_weight']}íšŒ")
                
                # 5. ì—°ê²° íŒ¨í„´ ë¶„ì„
                print(f"\nğŸ“ˆ ì—°ê²° íŒ¨í„´ ë¶„ì„:")
                
                # ì—°ê²° ê°•ë„ ë¶„í¬
                weight_distribution = company_connections.groupBy("weight").count().orderBy("weight").collect()
                print(f"   ì—°ê²° ê°•ë„ ë¶„í¬:")
                for row in weight_distribution:
                    print(f"     {row['weight']}íšŒ: {row['count']}ê°œ ì—°ê²°")
                
                # 6. ì˜í–¥ë ¥ ë¶„ì„
                print(f"\nğŸ’¡ ì˜í–¥ë ¥ ë¶„ì„:")
                
                # ë†’ì€ ì—°ê²° ê°•ë„ë¥¼ ê°€ì§„ ê´€ê³„ë“¤
                strong_connections = company_connections.filter(F.col("weight") >= 5).count()
                print(f"   ê°•í•œ ì—°ê²° (5íšŒ ì´ìƒ): {strong_connections}ê°œ")
                
                # ì—°ê²° ë‹¤ì–‘ì„±
                unique_partners = all_connections.select("partner").distinct().count()
                print(f"   ì—°ê²°ëœ ê¸°ì—… ìˆ˜: {unique_partners}ê°œ")
                print(f"   ì—°ê²° ë‹¤ì–‘ì„±: {unique_partners/total_connections*100:.1f}%")
                
            else:
                print(f"\nâŒ '{company_name}'ê³¼ ì—°ê²°ëœ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
                print("   ì´ ê¸°ì—…ì€ ë‰´ìŠ¤ì—ì„œ ë‹¤ë¥¸ KOSPI 200 ê¸°ì—…ê³¼ í•¨ê»˜ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸  ì—°ê²° ê´€ê³„ ë°ì´í„°ê°€ ì—†ì–´ ì—°ê²° ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        # 7. ìƒëŒ€ì  ìœ„ì¹˜ ë¶„ì„
        print(f"\nğŸ“Š ìƒëŒ€ì  ìœ„ì¹˜ ë¶„ì„:")
        
        # ìƒìœ„ 10% ê¸°ì—…ë“¤ê³¼ ë¹„êµ
        top_10_percent = int(total_companies * 0.1)
        top_companies = pagerank_results.limit(top_10_percent).collect()
        top_scores = [row['pagerank_score'] for row in top_companies]
        
        if company_score >= top_scores[-1]:
            print(f"   ğŸ† ìƒìœ„ 10% ê¸°ì—…ì— ì†í•©ë‹ˆë‹¤!")
        elif rank_position <= total_companies * 0.3:
            print(f"   ğŸ¥ˆ ìƒìœ„ 30% ê¸°ì—…ì— ì†í•©ë‹ˆë‹¤.")
        elif rank_position <= total_companies * 0.5:
            print(f"   ğŸ¥‰ ìƒìœ„ 50% ê¸°ì—…ì— ì†í•©ë‹ˆë‹¤.")
        else:
            print(f"   ğŸ“‰ í•˜ìœ„ 50% ê¸°ì—…ì— ì†í•©ë‹ˆë‹¤.")
        
        # 8. ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’­ ë¶„ì„ ê²°ê³¼:")
        if company_score > 0.01:
            print(f"   âœ… '{company_name}'ì€ ë†’ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“ˆ ë‰´ìŠ¤ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ë©° ë‹¤ë¥¸ ê¸°ì—…ë“¤ê³¼ ê°•í•œ ì—°ê²°ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        elif company_score > 0.005:
            print(f"   âš–ï¸  '{company_name}'ì€ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“Š ì ë‹¹í•œ ì—°ê²° ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"   âš ï¸  '{company_name}'ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“‰ ë‰´ìŠ¤ì—ì„œì˜ ì–¸ê¸‰ ë¹ˆë„ë‚˜ ì—°ê²° ê´€ê³„ê°€ ì œí•œì ì…ë‹ˆë‹¤.")
        
        # 9. S3 ì €ì¥ (ì„ íƒì‚¬í•­)
        save_analysis = input(f"\nğŸ’¾ ì´ ë¶„ì„ ê²°ê³¼ë¥¼ S3ì— ì €ì¥í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): ").strip().lower()
        if save_analysis == 'y':
            analysis_path = f"s3a://{s3_bucket}/{s3_prefix}/company_analysis/{company_name.replace(' ', '_')}/"
            
            # ë¶„ì„ ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
            analysis_data = [
                ("company_name", company_name),
                ("pagerank_score", company_score),
                ("rank_position", rank_position),
                ("total_companies", total_companies),
                ("analysis_date", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
            ]
            
            analysis_df = spark.createDataFrame(analysis_data, ["metric", "value"])
            analysis_df.write.mode("overwrite").parquet(analysis_path)
            print(f"âœ… ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {analysis_path}")
        
    except Exception as e:
        print(f"âŒ ê¸°ì—… ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def analyze_company(spark, company_name, pagerank_results, connections_df):
    """íŠ¹ì • ê¸°ì—…ì— ëŒ€í•œ ìƒì„¸ ë¶„ì„ (ë¡œì»¬ ë°ì´í„° ê¸°ë°˜)"""
    
    print(f"\nğŸ” '{company_name}' ê¸°ì—… ìƒì„¸ ë¶„ì„")
    print("=" * 60)
    
    try:
        # 1. PageRank ì ìˆ˜ í™•ì¸
        company_pagerank = pagerank_results.filter(
            F.col("company").contains(company_name)
        ).collect()
        
        if not company_pagerank:
            print(f"âŒ '{company_name}' ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("ğŸ’¡ ê¸°ì—…ëª…ì„ ì •í™•íˆ ì…ë ¥í•´ì£¼ì„¸ìš”. (ì˜ˆ: ì‚¼ì„±ì „ì, í˜„ëŒ€ì°¨)")
            return
        
        # 2. PageRank ì •ë³´
        company_score = company_pagerank[0]['pagerank_score']
        total_companies = pagerank_results.count()
        
        # ì „ì²´ ìˆœìœ„ ê³„ì‚°
        rank_position = pagerank_results.filter(
            F.col("pagerank_score") > company_score
        ).count() + 1
        
        print(f"ğŸ“Š ê¸°ë³¸ ì •ë³´:")
        print(f"   ê¸°ì—…ëª…: {company_pagerank[0]['company']}")
        print(f"   PageRank ì ìˆ˜: {company_score:.6f}")
        print(f"   ì „ì²´ ìˆœìœ„: {rank_position}ìœ„ / {total_companies}ê°œ ê¸°ì—…")
        print(f"   ìƒìœ„ {rank_position/total_companies*100:.1f}%")
        
        # 3. ì—°ê²° ê´€ê³„ ë¶„ì„
        company_connections = connections_df.filter(
            (F.col("company1").contains(company_name)) | 
            (F.col("company2").contains(company_name))
        )
        
        total_connections = company_connections.count()
        
        if total_connections > 0:
            avg_weight = company_connections.agg(F.avg("weight")).collect()[0][0]
            max_weight = company_connections.agg(F.max("weight")).collect()[0][0]
            
            print(f"\nğŸ”— ì—°ê²° ê´€ê³„ ë¶„ì„:")
            print(f"   ì´ ì—°ê²° ìˆ˜: {total_connections}ê°œ")
            print(f"   í‰ê·  ì—°ê²° ê°•ë„: {avg_weight:.1f}íšŒ")
            print(f"   ìµœëŒ€ ì—°ê²° ê°•ë„: {max_weight}íšŒ")
            
            # 4. ì£¼ìš” ì—°ê²° ê¸°ì—…ë“¤
            print(f"\nğŸ¤ ì£¼ìš” ì—°ê²° ê¸°ì—… TOP 10:")
            print("-" * 50)
            
            # company1ì´ ëŒ€ìƒ ê¸°ì—…ì¸ ê²½ìš°
            connections_as_source = company_connections.filter(
                F.col("company1").contains(company_name)
            ).select(
                F.col("company2").alias("partner"),
                F.col("weight")
            )
            
            # company2ê°€ ëŒ€ìƒ ê¸°ì—…ì¸ ê²½ìš°
            connections_as_target = company_connections.filter(
                F.col("company2").contains(company_name)
            ).select(
                F.col("company1").alias("partner"),
                F.col("weight")
            )
            
            # ëª¨ë“  ì—°ê²° í†µí•©
            all_connections = connections_as_source.union(connections_as_target)
            
            # íŒŒíŠ¸ë„ˆë³„ ì´ ì—°ê²° ê°•ë„ ê³„ì‚°
            partner_connections = all_connections.groupBy("partner").agg(
                F.sum("weight").alias("total_weight")
            ).orderBy(F.desc("total_weight")).limit(10)
            
            top_partners = partner_connections.collect()
            for i, row in enumerate(top_partners, 1):
                print(f"   {i:2d}. {row['partner']}: {row['total_weight']}íšŒ")
            
            # 5. ì—°ê²° íŒ¨í„´ ë¶„ì„
            print(f"\nğŸ“ˆ ì—°ê²° íŒ¨í„´ ë¶„ì„:")
            
            # ì—°ê²° ê°•ë„ ë¶„í¬
            weight_distribution = company_connections.groupBy("weight").count().orderBy("weight").collect()
            print(f"   ì—°ê²° ê°•ë„ ë¶„í¬:")
            for row in weight_distribution:
                print(f"     {row['weight']}íšŒ: {row['count']}ê°œ ì—°ê²°")
            
            # 6. ì˜í–¥ë ¥ ë¶„ì„
            print(f"\nğŸ’¡ ì˜í–¥ë ¥ ë¶„ì„:")
            
            # ë†’ì€ ì—°ê²° ê°•ë„ë¥¼ ê°€ì§„ ê´€ê³„ë“¤
            strong_connections = company_connections.filter(F.col("weight") >= 5).count()
            print(f"   ê°•í•œ ì—°ê²° (5íšŒ ì´ìƒ): {strong_connections}ê°œ")
            
            # ì—°ê²° ë‹¤ì–‘ì„±
            unique_partners = all_connections.select("partner").distinct().count()
            print(f"   ì—°ê²°ëœ ê¸°ì—… ìˆ˜: {unique_partners}ê°œ")
            print(f"   ì—°ê²° ë‹¤ì–‘ì„±: {unique_partners/total_connections*100:.1f}%")
            
        else:
            print(f"\nâŒ '{company_name}'ê³¼ ì—°ê²°ëœ ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            print("   ì´ ê¸°ì—…ì€ ë‰´ìŠ¤ì—ì„œ ë‹¤ë¥¸ KOSPI 200 ê¸°ì—…ê³¼ í•¨ê»˜ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 7. ê¶Œì¥ì‚¬í•­
        print(f"\nğŸ’­ ë¶„ì„ ê²°ê³¼:")
        if company_score > 0.01:
            print(f"   âœ… '{company_name}'ì€ ë†’ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“ˆ ë‰´ìŠ¤ì—ì„œ ìì£¼ ì–¸ê¸‰ë˜ë©° ë‹¤ë¥¸ ê¸°ì—…ë“¤ê³¼ ê°•í•œ ì—°ê²°ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        elif company_score > 0.005:
            print(f"   âš–ï¸  '{company_name}'ì€ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“Š ì ë‹¹í•œ ì—°ê²° ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"   âš ï¸  '{company_name}'ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì€ ì˜í–¥ë ¥ì„ ê°€ì§„ ê¸°ì—…ì…ë‹ˆë‹¤.")
            print(f"   ğŸ“‰ ë‰´ìŠ¤ì—ì„œì˜ ì–¸ê¸‰ ë¹ˆë„ë‚˜ ì—°ê²° ê´€ê³„ê°€ ì œí•œì ì…ë‹ˆë‹¤.")
        
    except Exception as e:
        print(f"âŒ ê¸°ì—… ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì™„ì „ í•¨ìˆ˜í˜•)"""
    
    print("ğŸš€ ë‹¨ìˆœí™”ëœ KOSPI 200 PageRank ë¶„ì„ê¸°")
    print("=" * 50)
    
    # ë°ì´í„° ì†ŒìŠ¤ í™•ì¸ (S3 ë˜ëŠ” ë¡œì»¬)
    s3_bucket = os.getenv("S3_BUCKET")
    s3_prefix = os.getenv("S3_PREFIX", "data")
    
    if s3_bucket:
        # S3ì—ì„œ ë°ì´í„° ì½ê¸°
        s3_data_path = f"s3a://{s3_bucket}/{s3_prefix}/data/*.csv"
        print(f"â˜ï¸ S3 ë°ì´í„° ì†ŒìŠ¤: {s3_data_path}")
        csv_file = s3_data_path
    else:
        # ë¡œì»¬ ë°ì´í„° ì½ê¸°
        csv_files = ["/opt/spark/data/*.csv"]
        
        print("ğŸ“ ë¡œì»¬ CSV íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        csv_file = None
        for file in csv_files:
            if any(ch in file for ch in ['*', '?', '[']):
                matched = glob.glob(file)
                if matched:
                    csv_file = file
                    print(f"âœ… ë§¤ì¹­ëœ íŒŒì¼ {len(matched)}ê°œ: {file}")
                    break
        
        if csv_file is None:
            print("âŒ ë¡œì»¬ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            print("ğŸ’¡ S3 ì‚¬ìš©: docker-compose.ymlì— S3_BUCKET í™˜ê²½ë³€ìˆ˜ ì„¤ì •")
            return
    
    # Spark ì„¸ì…˜ ì´ˆê¸°í™”
    spark = init_spark_session()
    
    try:
        # 1. ë°ì´í„° ë¡œë“œ (CSV ë˜ëŠ” Excel)
        news_df = load_data(spark, csv_file)
        if news_df is None:
            return
        
        # 2. ì—°ê²° ê´€ê³„ ì¶”ì¶œ
        connections_df = extract_kospi200_connections(spark, news_df)
        if connections_df is None:
            return
        
        # 3. PageRank ê³„ì‚°
        pagerank_results = calculate_pagerank(spark, connections_df)
        if pagerank_results is None:
            return
        
        print(f"\nğŸ‰ KOSPI 200 ë¶„ì„ ì™„ë£Œ!")
        
        # S3 ì €ì¥ (í™˜ê²½ë³€ìˆ˜ S3_BUCKET/S3_PREFIX ì„¤ì • ì‹œ)
        bucket = os.getenv("S3_BUCKET")
        prefix = os.getenv("S3_PREFIX", "outputs/pagerank").rstrip("/")
        if bucket:
            base = f"s3a://{bucket}/{prefix}"
            try:
                print(f"\nâ˜ï¸  S3 ì €ì¥ ì¤‘: {base}")
                pagerank_results.write.mode("overwrite").parquet(f"{base}/pagerank/")
                connections_df.write.mode("overwrite").parquet(f"{base}/connections/")
                print("âœ… S3 Parquet ì €ì¥ ì™„ë£Œ")
                if os.getenv("EXPORT_CSV", "false").lower() in ("1","true","yes"):
                    pagerank_results.coalesce(1).write.mode("overwrite").option("header","true").csv(f"{base}/pagerank_csv/")
                    print("âœ… S3 CSV ë‚´ë³´ë‚´ê¸° ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ S3 ì €ì¥ ì‹¤íŒ¨: {e}")
        
        total_companies = pagerank_results.count()
        print(f"\nğŸ“ˆ ë¶„ì„ ìš”ì•½:")
        print(f"   ë¶„ì„ëœ ê¸°ì—… ìˆ˜: {total_companies}ê°œ")
        
        top_3 = pagerank_results.limit(3).collect()
        print(f"   TOP 3 ì˜í–¥ë ¥ ê¸°ì—…:")
        for i, row in enumerate(top_3, 1):
            print(f"     {i}. {row['company']} (ì ìˆ˜: {row['pagerank_score']:.6f})")
        
        # # ê¸°ì—… ìƒì„¸ ë¶„ì„ (S3 ê¸°ë°˜)
        # print(f"\n" + "="*60)
        # print("ğŸ” ê¸°ì—… ìƒì„¸ ë¶„ì„ ëª¨ë“œ (S3 ê¸°ë°˜)")
        # print("="*60)
        
        # # S3 ì„¤ì • í™•ì¸
        # s3_bucket = os.getenv("S3_BUCKET")
        # s3_prefix = os.getenv("S3_PREFIX", "outputs/pagerank")
        
        # if s3_bucket:
        #     print(f"â˜ï¸ S3 ê¸°ë°˜ ë¶„ì„ ëª¨ë“œ")
        #     print(f"   ë²„í‚·: {s3_bucket}")
        #     print(f"   ê²½ë¡œ: {s3_prefix}")
            
        #     while True:
        #         try:
        #             company_input = input("\nğŸ“ ë¶„ì„í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'): ").strip()
                    
        #             if company_input.lower() in ['quit', 'exit', 'q']:
        #                 print("ğŸ‘‹ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #                 break
                    
        #             if not company_input:
        #                 print("âš ï¸  ê¸°ì—…ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        #                 continue
                    
        #             # S3 ê¸°ë°˜ ê¸°ì—… ë¶„ì„ ì‹¤í–‰
        #             analyze_company_from_s3(spark, company_input, s3_bucket, s3_prefix)
                    
        #         except KeyboardInterrupt:
        #             print("\nğŸ‘‹ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #             break
        #         except Exception as e:
        #             print(f"âŒ ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        #             continue
        # else:
        #     print(f"âš ï¸  S3_BUCKET í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ S3 ê¸°ë°˜ ë¶„ì„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        #     print(f"ğŸ’¡ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ë¡œì»¬ ë¶„ì„ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            
        #     # ë¡œì»¬ ë¶„ì„ ëª¨ë“œ (ê¸°ì¡´ ë°©ì‹)
        #     print(f"\nğŸ” ë¡œì»¬ ë¶„ì„ ëª¨ë“œ")
        #     while True:
        #         try:
        #             company_input = input("\nğŸ“ ë¶„ì„í•  ê¸°ì—…ëª…ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit'): ").strip()
                    
        #             if company_input.lower() in ['quit', 'exit', 'q']:
        #                 print("ğŸ‘‹ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #                 break
                    
        #             if not company_input:
        #                 print("âš ï¸  ê¸°ì—…ëª…ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        #                 continue
                    
        #             # ë¡œì»¬ ê¸°ì—… ë¶„ì„ ì‹¤í–‰ (ê¸°ì¡´ í•¨ìˆ˜ ì‚¬ìš©)
        #             analyze_company(spark, company_input, pagerank_results, connections_df)
                    
        #         except KeyboardInterrupt:
        #             print("\nğŸ‘‹ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        #             break
        #         except Exception as e:
        #             print(f"âŒ ì…ë ¥ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        #             continue
    
    finally:
        print("ğŸ”„ Spark ì„¸ì…˜ ì¢…ë£Œ ì¤‘...")
        spark.stop()
        print("âœ… ì™„ë£Œ")

if __name__ == "__main__":
    main()