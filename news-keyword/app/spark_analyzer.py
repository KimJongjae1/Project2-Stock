"""
PySparkë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì—”ì§„
ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”
"""

import os
import re
from typing import Dict, List
from collections import Counter
import logging

logger = logging.getLogger(__name__)

class SparkAnalyzer:
    """PySparkë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¶„ì„ê¸°"""
    
    def __init__(self, spark_session, s3_bucket: str, s3_prefix: str):
        self.spark = spark_session
        self.s3_bucket = s3_bucket
        self.s3_prefix = s3_prefix
    
    def extract_keywords_with_spark(self, company_name: str, start_date: str, end_date: str, top_keywords: int, csv_files: List[str]) -> Dict:
        """
        PySparkë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ëŒ€ìš©ëŸ‰ ë°ì´í„°ìš©)
        """
        try:
            if self.spark is None:
                raise Exception("SparkSessionì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            logger.info(f"ğŸš€ PySpark ì—”ì§„ìœ¼ë¡œ CSV íŒŒì¼ë“¤ ì½ê¸° ì‹œì‘: {len(csv_files)}ê°œ íŒŒì¼")
            
            # ëª¨ë“  CSV íŒŒì¼ ì½ê¸° ë° ë³‘í•©
            dataframes = []
            for csv_path in csv_files:
                try:
                    logger.info(f"íŒŒì¼ ì½ëŠ” ì¤‘: {os.path.basename(csv_path)}")
                    temp_df = self.spark.read \
                        .option("header", "true") \
                        .option("inferSchema", "true") \
                        .option("encoding", "UTF-8") \
                        .option("multiline", "true") \
                        .option("escape", '"') \
                        .csv(csv_path)
                    
                    dataframes.append(temp_df)
                    logger.info(f"  - ë¡œë“œëœ í–‰ ìˆ˜: {temp_df.count()}")
                    
                except Exception as e:
                    logger.warning(f"CSV íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {csv_path}, ì˜¤ë¥˜: {e}")
                    continue
            
            if not dataframes:
                raise FileNotFoundError("ì½ì„ ìˆ˜ ìˆëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
            df = dataframes[0]
            for temp_df in dataframes[1:]:
                df = df.union(temp_df)
            
            total_rows = df.count()
            logger.info(f"ì´ {len(csv_files)}ê°œ íŒŒì¼ì—ì„œ {total_rows}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
            logger.info(f"ì»¬ëŸ¼ëª…: {df.columns}")
            
            # ê¸°ì—… í•„í„°ë§ (ê¸°ê´€ ì»¬ëŸ¼ì—ì„œ í•´ë‹¹ ê¸°ì—…ì´ í¬í•¨ëœ í–‰ë“¤ì„ ê°€ì ¸ì˜´)
            if 'ê¸°ê´€' in df.columns:
                # ê¸°ê´€ ì»¬ëŸ¼ì— NaNì´ ì•„ë‹ˆê³  íšŒì‚¬ëª…ì´ í¬í•¨ëœ í–‰ í•„í„°ë§
                company_filtered_df = df.filter(
                    (df['ê¸°ê´€'].isNotNull()) & 
                    (df['ê¸°ê´€'].contains(company_name))
                )
                company_count = company_filtered_df.count()
                
                logger.info(f"'{company_name}' ê´€ë ¨ ë‰´ìŠ¤: {company_count}ê°œ (ê¸°ê´€ í•„í„°ë§ í›„)")
                
                # ë‚ ì§œ í•„í„°ë§ ì ìš©
                date_filtered_df = self.apply_date_filter(company_filtered_df, start_date, end_date)
                total_count = date_filtered_df.count()
                
                logger.info(f"ë‚ ì§œ í•„í„°ë§ í›„ ë‰´ìŠ¤: {total_count}ê°œ ({start_date}-{end_date})")
                
                # ìµœì¢… í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©
                filtered_df = date_filtered_df
                
                if total_count == 0:
                    return {
                        "company_name": company_name,
                        "period": f"{start_date}-{end_date}",
                        "total_news_count": 0,
                        "daily_news_count": {},
                        "keywords": {},
                        "message": f"'{company_name}'ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }
                
                # ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
                daily_news_count = self.calculate_daily_news_count(filtered_df, start_date, end_date)
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ í‚¤ì›Œë“œ ì»¬ëŸ¼ ì‚¬ìš©)
                if 'í‚¤ì›Œë“œ' in df.columns:
                    # í‚¤ì›Œë“œ ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œ ë¶„ë¦¬ ë° ì •ë¦¬
                    keyword_rows = filtered_df.select("í‚¤ì›Œë“œ").filter(df['í‚¤ì›Œë“œ'].isNotNull()).collect()
                    
                    all_keywords = []
                    for row in keyword_rows:
                        keywords_str = row['í‚¤ì›Œë“œ']
                        if keywords_str:
                            keywords = [re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', k.strip()) for k in keywords_str.split(',') if k.strip()]
                            all_keywords.extend([k for k in keywords if len(k) >= 2])
                    
                    # ê¸°ì—…ëª… ìì²´ëŠ” í‚¤ì›Œë“œì—ì„œ ì œì™¸
                    all_keywords = [k for k in all_keywords if company_name not in k]
                    
                    # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                    keyword_counter = Counter(all_keywords)
                    
                    # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                    keywords_dict = dict(keyword_counter.most_common())
                    
                    # ìƒìœ„ í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ ì¶”ì¶œ
                    top_keywords_list = list(keywords_dict.keys())[:top_keywords]
                    top_news_articles = self.extract_top_news_articles(filtered_df, top_keywords_list)
                    
                    logger.info(f"ğŸš€ PySpark ì—”ì§„ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {len(keywords_dict)}ê°œ í‚¤ì›Œë“œ")
                    
                    return {
                        "company_name": company_name,
                        "period": f"{start_date}-{end_date}",
                        "total_news_count": total_count,
                        "daily_news_count": daily_news_count,
                        "keywords": keywords_dict,
                        "top_news_articles": top_news_articles,
                        "message": f"ğŸš€ PySpark ì—”ì§„ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤. ì´ {len(keywords_dict)}ê°œ í‚¤ì›Œë“œ ë°œê²¬ (íŒŒì¼ {len(csv_files)}ê°œ ì²˜ë¦¬)"
                    }
                else:
                    return {
                        "company_name": company_name,
                        "period": f"{start_date}-{end_date}",
                        "total_news_count": total_count,
                        "daily_news_count": daily_news_count,
                        "keywords": {},
                        "message": "í‚¤ì›Œë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    }
            else:
                raise ValueError("ê¸°ê´€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            logger.error(f"PySparkë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            raise e
    
    def apply_date_filter(self, df, start_date: str, end_date: str):
        """
        ë‚ ì§œ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì„¤ì •ëœ ê¸°ê°„ ë‚´ì˜ ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            df: í•„í„°ë§í•  Spark DataFrame
            start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            
        Returns:
            Spark DataFrame: ë‚ ì§œ í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            from datetime import datetime
            from pyspark.sql.functions import col, to_date, when, isnan, isnull
            
            # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            date_column = None
            for col_name in ['ì¼ì', 'ë‚ ì§œ', 'date', 'Date', 'DATE']:
                if col_name in df.columns:
                    date_column = col_name
                    break
            
            if date_column is None:
                logger.warning("ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return df
            
            logger.info(f"ë‚ ì§œ í•„í„°ë§ì— ì‚¬ìš©í•  ì»¬ëŸ¼: {date_column}")
            
            # ë‚ ì§œ ë²”ìœ„ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            start_date_str = f"{start_date[:4]}-{start_date[4:6]}-{start_date[6:8]}"
            end_date_str = f"{end_date[:4]}-{end_date[4:6]}-{end_date[6:8]}"
            
            # ë‚ ì§œ ì»¬ëŸ¼ì„ ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ íŒŒì‹± ì‹œë„
            date_col = col(date_column)
            
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì‹œë„
            parsed_date = when(
                date_col.rlike(r'^\d{8}$'),  # YYYYMMDD í˜•ì‹
                to_date(date_col, 'yyyyMMdd')
            ).when(
                date_col.rlike(r'^\d{4}-\d{2}-\d{2}$'),  # YYYY-MM-DD í˜•ì‹
                to_date(date_col, 'yyyy-MM-dd')
            ).when(
                date_col.rlike(r'^\d{4}/\d{2}/\d{2}$'),  # YYYY/MM/DD í˜•ì‹
                to_date(date_col, 'yyyy/MM/dd')
            ).when(
                date_col.rlike(r'^\d{4}\.\d{2}\.\d{2}$'),  # YYYY.MM.DD í˜•ì‹
                to_date(date_col, 'yyyy.MM.dd')
            ).otherwise(None)
            
            # ë‚ ì§œ í•„í„°ë§ ì ìš©
            filtered_df = df.filter(
                parsed_date.isNotNull() &
                (parsed_date >= start_date_str) &
                (parsed_date <= end_date_str)
            )
            
            filtered_count = filtered_df.count()
            logger.info(f"ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì™„ë£Œ: {filtered_count}ê°œ ë‰´ìŠ¤")
            
            # ìƒ˜í”Œ ë‚ ì§œ ê°’ ë¡œê·¸ ì¶œë ¥
            if filtered_count > 0:
                sample_dates = filtered_df.select(date_column).limit(3).collect()
                sample_values = [row[date_column] for row in sample_dates]
                logger.info(f"í•„í„°ë§ëœ ìƒ˜í”Œ ë‚ ì§œ: {sample_values}")
            
            return filtered_df
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.info("ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆë›°ê³  ì›ë³¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return df
    
    def calculate_daily_news_count(self, filtered_df, start_date: str, end_date: str) -> Dict[str, int]:
        """
        ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            filtered_df: í•„í„°ë§ëœ Spark DataFrame
            start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            
        Returns:
            Dict[str, int]: ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ {"20210811": 15, "20210812": 23, ...}
        """
        try:
            from datetime import datetime, timedelta
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ í™•ì¸
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {filtered_df.columns}")
            
            # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            date_column = None
            for col in ['ì¼ì', 'ë‚ ì§œ', 'date', 'Date', 'DATE']:
                if col in filtered_df.columns:
                    date_column = col
                    break
            
            if date_column is None:
                logger.warning("ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return {}
            
            logger.info(f"ë‚ ì§œ ì»¬ëŸ¼ ì‚¬ìš©: {date_column}")
            
            # ë‚ ì§œ ë²”ìœ„ ìƒì„±
            start_dt = datetime.strptime(start_date, "%Y%m%d")
            end_dt = datetime.strptime(end_date, "%Y%m%d")
            
            daily_count = {}
            current_date = start_dt
            
            # ê° ë‚ ì§œë³„ë¡œ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
            while current_date <= end_dt:
                date_str = current_date.strftime("%Y%m%d")
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
                try:
                    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
                    date_patterns = [
                        date_str,  # 20210811
                        f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}",  # 2021-08-11
                        f"{date_str[:4]}/{date_str[4:6]}/{date_str[6:8]}",  # 2021/08/11
                        f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:8]}",  # 2021.08.11
                    ]
                    
                    # ê° íŒ¨í„´ì— ëŒ€í•´ ë§¤ì¹­ ì‹œë„
                    count = 0
                    for pattern in date_patterns:
                        date_count_df = filtered_df.filter(
                            filtered_df[date_column].cast("string").contains(pattern)
                        )
                        pattern_count = date_count_df.count()
                        if pattern_count > 0:
                            count = pattern_count
                            logger.debug(f"{date_str} ({pattern}): {count}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                            break
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
                    if count == 0:
                        # ìƒ˜í”Œ ë‚ ì§œ ê°’ í™•ì¸
                        sample_dates = filtered_df.select(date_column).filter(
                            filtered_df[date_column].isNotNull()
                        ).limit(3).collect()
                        sample_values = [row[date_column] for row in sample_dates]
                        logger.debug(f"{date_str} ë§¤ì¹­ ì‹¤íŒ¨. ìƒ˜í”Œ ë‚ ì§œ ê°’: {sample_values}")
                    
                except Exception as e:
                    logger.warning(f"ë‚ ì§œ {date_str} ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                    count = 0
                
                daily_count[date_str] = int(count)
                current_date += timedelta(days=1)
            
            # ì´í•© ê²€ì¦
            total_daily_count = sum(daily_count.values())
            logger.info(f"ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(daily_count)}ì¼, ì´í•©: {total_daily_count}ê°œ")
            logger.info(f"daily_news_count í•©ê³„: {total_daily_count}, total_news_count: {filtered_df.count()}")
            
            return daily_count
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def extract_top_news_articles(self, filtered_df, top_keywords_list, max_articles=10):
        """
        ìƒìœ„ í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            filtered_df: í•„í„°ë§ëœ Spark DataFrame
            top_keywords_list: ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not top_keywords_list:
                return []
            
            # í•„ìš”í•œ ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
            required_columns = ['ì œëª©', 'ë‚ ì§œ', 'URL', 'í‚¤ì›Œë“œ']
            available_columns = [col for col in required_columns if col in filtered_df.columns]
            
            if len(available_columns) < 3:  # ìµœì†Œ 3ê°œ ì»¬ëŸ¼ í•„ìš”
                logger.warning(f"í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {available_columns}")
                return []
            
            # ê° ê¸°ì‚¬ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°
            articles_with_score = []
            
            # DataFrameì„ collectí•˜ì—¬ Pythonì—ì„œ ì²˜ë¦¬
            articles_data = filtered_df.select(*available_columns).collect()
            
            for row in articles_data:
                article_keywords = []
                if row.get('í‚¤ì›Œë“œ') and str(row['í‚¤ì›Œë“œ']).strip():
                    # ê¸°ì‚¬ í‚¤ì›Œë“œ ë¶„ë¦¬
                    article_keywords = [k.strip() for k in str(row['í‚¤ì›Œë“œ']).split(',') if k.strip()]
                
                # ìƒìœ„ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” ê°œìˆ˜ ê³„ì‚°
                matched_count = 0
                matched_keywords = []
                for keyword in top_keywords_list:
                    for article_keyword in article_keywords:
                        if keyword in article_keyword or article_keyword in keyword:
                            matched_count += 1
                            matched_keywords.append(keyword)
                            break  # ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€
                
                if matched_count > 0:
                    # nan ê°’ ì²˜ë¦¬
                    title = row.get('ì œëª©', 'ì œëª© ì—†ìŒ')
                    if title is None or str(title).lower() == 'nan':
                        title = 'ì œëª© ì—†ìŒ'
                    
                    date = row.get('ì¼ì', 'ì¼ì ì—†ìŒ')
                    if date is None or str(date).lower() == 'nan':
                        date = 'ì¼ì ì—†ìŒ'
                    
                    url = row.get('URL', 'URL ì—†ìŒ')
                    if url is None or str(url).lower() == 'nan':
                        url = 'URL ì—†ìŒ'
                    
                    articles_with_score.append({
                        'title': str(title),
                        'date': str(date),
                        'url': str(url),
                        'matched_keywords_count': matched_count,
                        'matched_keywords': list(set(matched_keywords)),
                        'all_keywords': article_keywords
                    })
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            articles_with_score.sort(key=lambda x: x['matched_keywords_count'], reverse=True)
            
            # ìƒìœ„ ê¸°ì‚¬ë“¤ë§Œ ë°˜í™˜
            top_articles = articles_with_score[:max_articles]
            
            logger.info(f"ìƒìœ„ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ {len(top_articles)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            
            return top_articles
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
