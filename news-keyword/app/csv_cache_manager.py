#!/usr/bin/env python3
"""
CSV íŒŒì¼ ë””ìŠ¤í¬ ìºì‹œ ë§¤ë‹ˆì €
S3ì—ì„œ ì½ì€ CSV íŒŒì¼ì„ ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥í•˜ê³  ì¬ì‚¬ìš©
"""

import os
import hashlib
import logging
import pandas as pd
import time
from typing import Optional
from pathlib import Path

logger = logging.getLogger(__name__)

class CSVCacheManager:
    """CSV íŒŒì¼ ë””ìŠ¤í¬ ìºì‹œ ê´€ë¦¬"""
    
    def __init__(self, cache_dir: str = "./csv_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        
        # ìºì‹œ í†µê³„
        self.cache_hits = 0
        self.cache_misses = 0
        
        logger.info(f"CSV ìºì‹œ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”: {self.cache_dir.absolute()}")
    
    def _get_cache_key(self, s3_path: str) -> str:
        """S3 ê²½ë¡œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        # S3 ê²½ë¡œë¥¼ í•´ì‹œí™”í•˜ì—¬ ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
        hash_key = hashlib.md5(s3_path.encode()).hexdigest()
        filename = os.path.basename(s3_path)
        return f"{hash_key}_{filename}"
    
    def _get_cache_path(self, cache_key: str) -> Path:
        """ìºì‹œ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
        return self.cache_dir / cache_key
    
    def is_cached(self, s3_path: str) -> bool:
        """íŒŒì¼ì´ ìºì‹œë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        cache_key = self._get_cache_key(s3_path)
        cache_path = self._get_cache_path(cache_key)
        return cache_path.exists()
    
    def save_to_cache(self, s3_path: str, df: pd.DataFrame) -> bool:
        """DataFrameì„ ìºì‹œì— ì €ì¥"""
        try:
            cache_key = self._get_cache_key(s3_path)
            cache_path = self._get_cache_path(cache_key)
            
            start_time = time.time()
            
            # Parquet í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ì••ì¶•ë¥ ê³¼ ì†ë„ ìµœì í™”)
            parquet_path = cache_path.with_suffix('.parquet')
            df.to_parquet(
                parquet_path,
                engine='pyarrow',
                compression='snappy',
                index=False
            )
            
            save_time = time.time() - start_time
            file_size = parquet_path.stat().st_size / (1024*1024)  # MB
            
            logger.info(f"ğŸ’¾ ìºì‹œ ì €ì¥ ì™„ë£Œ: {os.path.basename(s3_path)}")
            logger.info(f"   ğŸ“ í¬ê¸°: {file_size:.2f}MB, ì‹œê°„: {save_time:.3f}ì´ˆ")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ({s3_path}): {e}")
            return False
    
    def load_from_cache(self, s3_path: str) -> Optional[pd.DataFrame]:
        """ìºì‹œì—ì„œ DataFrame ë¡œë“œ"""
        try:
            cache_key = self._get_cache_key(s3_path)
            cache_path = self._get_cache_path(cache_key)
            parquet_path = cache_path.with_suffix('.parquet')
            
            if not parquet_path.exists():
                self.cache_misses += 1
                return None
            
            start_time = time.time()
            df = pd.read_parquet(parquet_path, engine='pyarrow')
            load_time = time.time() - start_time
            
            self.cache_hits += 1
            
            logger.info(f"ğŸ“‚ ìºì‹œì—ì„œ ë¡œë“œ: {os.path.basename(s3_path)}")
            logger.info(f"   ğŸ“Š í–‰ ìˆ˜: {len(df):,}ê°œ, ì‹œê°„: {load_time:.3f}ì´ˆ")
            
            return df
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨ ({s3_path}): {e}")
            self.cache_misses += 1
            return None
    
    def get_cache_stats(self) -> dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = (self.cache_hits / total_requests * 100) if total_requests > 0 else 0
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚°
        total_size = 0
        file_count = 0
        for file_path in self.cache_dir.glob('*.parquet'):
            total_size += file_path.stat().st_size
            file_count += 1
        
        return {
            "cache_hits": self.cache_hits,
            "cache_misses": self.cache_misses,
            "hit_rate": f"{hit_rate:.1f}%",
            "cached_files": file_count,
            "total_size_mb": f"{total_size / (1024*1024):.2f}MB"
        }
    
    def clear_cache(self) -> bool:
        """ìºì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬"""
        try:
            deleted_count = 0
            for file_path in self.cache_dir.glob('*.parquet'):
                file_path.unlink()
                deleted_count += 1
            
            logger.info(f"ğŸ—‘ï¸ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return False
    
    def cleanup_old_cache(self, max_age_days: int = 7) -> int:
        """ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ ì •ë¦¬"""
        try:
            current_time = time.time()
            max_age_seconds = max_age_days * 24 * 3600
            deleted_count = 0
            
            for file_path in self.cache_dir.glob('*.parquet'):
                file_age = current_time - file_path.stat().st_mtime
                if file_age > max_age_seconds:
                    file_path.unlink()
                    deleted_count += 1
            
            if deleted_count > 0:
                logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬: {deleted_count}ê°œ íŒŒì¼ ì‚­ì œ ({max_age_days}ì¼ ì´ìƒ)")
            
            return deleted_count
            
        except Exception as e:
            logger.error(f"âŒ ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
            return 0
    
    def print_cache_stats(self):
        """ìºì‹œ í†µê³„ ì¶œë ¥"""
        stats = self.get_cache_stats()
        logger.info("ğŸ“Š === CSV ìºì‹œ í†µê³„ ===")
        logger.info(f"   ğŸ¯ ìºì‹œ ì ì¤‘ë¥ : {stats['hit_rate']}")
        logger.info(f"   âœ… ìºì‹œ íˆíŠ¸: {stats['cache_hits']}íšŒ")
        logger.info(f"   âŒ ìºì‹œ ë¯¸ìŠ¤: {stats['cache_misses']}íšŒ")
        logger.info(f"   ğŸ“ ìºì‹œëœ íŒŒì¼: {stats['cached_files']}ê°œ")
        logger.info(f"   ğŸ’¾ ì´ í¬ê¸°: {stats['total_size_mb']}")
