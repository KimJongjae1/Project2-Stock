from typing import Optional, Dict, List
import os
import logging
import pandas as pd
import re
import glob
import boto3
from datetime import datetime, timedelta
from collections import Counter
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, split, explode, count, collect_list, when, size, slice, regexp_replace, trim, length, lower
from smart_keyword_filter import SmartKeywordFilter
from spark_analyzer import SparkAnalyzer
from pandas_analyzer import PandasAnalyzer

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class KeywordExtractor:
    """PySparkë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.spark = None
        self.csv_file_path = None
        self.smart_filter = SmartKeywordFilter()
        
        # S3 ì„¤ì •
        self.s3_bucket = os.getenv('S3_BUCKET', 'cheesecrust-spark-data-bucket')
        self.s3_prefix = os.getenv('S3_PREFIX', 'outputs/data/')
        self.s3_region = os.getenv('AWS_DEFAULT_REGION', 'ap-northeast-2')
        
        # S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.s3_client = boto3.client(
            's3',
            region_name=self.s3_region,
            aws_access_key_id=os.getenv('AWS_ACCESS_KEY_ID'),
            aws_secret_access_key=os.getenv('AWS_SECRET_ACCESS_KEY'),
            aws_session_token=os.getenv('AWS_SESSION_TOKEN')
        )
        
        # ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.pandas_analyzer = PandasAnalyzer()
        self.spark_analyzer = None  # Spark ì´ˆê¸°í™” í›„ ì„¤ì •
        
    def initialize_spark(self):
        """SparkSession ì´ˆê¸°í™” (Java 11ê³¼ PySpark 3.3.0 í˜¸í™˜ì„± ìµœì í™”)"""
        if self.spark is None:
            try:
                import os
                
                # Java í™˜ê²½ ë³€ìˆ˜ í™•ì¸ ë° ì„¤ì • (Java 8 ì‚¬ìš©)
                java_home = os.environ.get('JAVA_HOME')
                if not java_home:
                    os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'
                    logger.info(f"JAVA_HOME ì„¤ì •: {os.environ['JAVA_HOME']}")
                
                # SPARK_HOME í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
                spark_home = os.environ.get('SPARK_HOME')
                if not spark_home:
                    os.environ['SPARK_HOME'] = '/usr/local/lib/python3.9/dist-packages/pyspark'
                    logger.info(f"SPARK_HOME ì„¤ì •: {os.environ['SPARK_HOME']}")
                
                # PySpark Python ì‹¤í–‰ íŒŒì¼ ì„¤ì •
                os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'
                os.environ['PYSPARK_DRIVER_PYTHON'] = '/usr/bin/python'
                
                print(f"AWS_ACCESS_KEY_ID: {os.getenv('AWS_ACCESS_KEY_ID', 'NOT_SET')}")
                print(f"AWS_SECRET_ACCESS_KEY: {'SET' if os.getenv('AWS_SECRET_ACCESS_KEY') else 'NOT_SET'}")
                
                self.spark = SparkSession.builder \
                    .appName("NewsKeywordAPI") \
                    .master("local[*]") \
                    .config("spark.driver.memory", "2g") \
                    .config("spark.driver.maxResultSize", "1g") \
                    .config("spark.executor.memory", "2g") \
                    .config("spark.sql.adaptive.enabled", "true") \
                    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                    .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
                    .config("spark.sql.execution.arrow.pyspark.enabled", "false") \
                    .config("spark.sql.execution.arrow.enabled", "false") \
                    .config("spark.sql.shuffle.partitions", "200") \
                    .config("spark.default.parallelism", "4") \
                    .config("spark.driver.host", "localhost") \
                    .config("spark.driver.bindAddress", "0.0.0.0") \
                    .config("spark.ui.enabled", "false") \
                    .config("spark.ui.showConsoleProgress", "false") \
                    .config("spark.jars.packages", 
                            "org.apache.hadoop:hadoop-aws:3.3.4,"
                            "com.amazonaws:aws-java-sdk-bundle:1.12.262") \
                    .config("spark.hadoop.fs.s3a.access.key", os.getenv('AWS_ACCESS_KEY_ID')) \
                    .config("spark.hadoop.fs.s3a.secret.key", os.getenv('AWS_SECRET_ACCESS_KEY')) \
                    .config("spark.hadoop.fs.s3a.endpoint", f"s3.{self.s3_region}.amazonaws.com") \
                    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
                    .getOrCreate()
                
                # ë¡œê·¸ ë ˆë²¨ ì„¤ì • (ë„ˆë¬´ ë§ì€ ë¡œê·¸ ë°©ì§€)
                self.spark.sparkContext.setLogLevel("WARN")
                
                # Java ë²„ì „ í™•ì¸
                java_version = self.spark.sparkContext._jvm.System.getProperty("java.version")
                logger.info(f"SparkSession ì´ˆê¸°í™” ì„±ê³µ! Java ë²„ì „: {java_version}")
                
                # SparkAnalyzer ì´ˆê¸°í™”
                self.spark_analyzer = SparkAnalyzer(self.spark, self.s3_bucket, self.s3_prefix)
                
            except Exception as e:
                logger.error(f"SparkSession ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                logger.info("Java ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
                # pandas ë°±ì—… í”Œëœ ì‚¬ìš©ì„ ìœ„í•´ sparkë¥¼ Noneìœ¼ë¡œ ìœ ì§€
                self.spark = None
                raise
    
    def find_csv_files(self, start_date: str, end_date: str) -> List[str]:
        """
        ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ë“¤ì„ S3ì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
        S3 ë²„í‚·ì—ì„œ í•´ë‹¹ ê¸°ê°„ì˜ ëª¨ë“  CSV íŒŒì¼ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        
        # ì…ë ¥ ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
        start_dt = datetime.strptime(start_date, "%Y%m%d")
        end_dt = datetime.strptime(end_date, "%Y%m%d")
        
        matching_files = []
        
        try:
            # S3ì—ì„œ ê°ì²´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            paginator = self.s3_client.get_paginator('list_objects_v2')
            page_iterator = paginator.paginate(
                Bucket=self.s3_bucket,
                Prefix=self.s3_prefix
            )
            
            for page in page_iterator:
                if 'Contents' in page:
                    for obj in page['Contents']:
                        key = obj['Key']
                        
                        # CSV íŒŒì¼ë§Œ í•„í„°ë§
                        if key.endswith('.csv'):
                            filename = os.path.basename(key)
                            
                            try:
                                # NewsResult_YYYYMMDD-YYYYMMDD.csv í˜•ì‹ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                                if filename.startswith('NewsResult_'):
                                    date_part = filename.replace("NewsResult_", "").replace(".csv", "")
                                    
                                    if "-" in date_part:
                                        file_start_str, file_end_str = date_part.split("-")
                                        file_start_dt = datetime.strptime(file_start_str, "%Y%m%d")
                                        file_end_dt = datetime.strptime(file_end_str, "%Y%m%d")
                                        
                                        # ë‚ ì§œ ë²”ìœ„ê°€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                                        if (file_start_dt <= end_dt and file_end_dt >= start_dt):
                                            s3_path = f"s3a://{self.s3_bucket}/{key}"
                                            matching_files.append(s3_path)
                                            logger.info(f"ë§¤ì¹­ëœ S3 íŒŒì¼: {filename} ({file_start_str}-{file_end_str})")
                                            
                            except ValueError as e:
                                # ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê±´ë„ˆë›°ê¸°
                                logger.debug(f"ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, íŒŒì¼ ê±´ë„ˆë›°ê¸°: {filename}")
                                continue
                                
        except Exception as e:
            logger.error(f"S3ì—ì„œ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise FileNotFoundError(f"S3ì—ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        
        if not matching_files:
            raise FileNotFoundError(f"ë‚ ì§œ ë²”ìœ„ {start_date}-{end_date}ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì„ S3ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        matching_files.sort()  # íŒŒì¼ëª… ìˆœìœ¼ë¡œ ì •ë ¬
        logger.info(f"ì´ {len(matching_files)}ê°œì˜ CSV íŒŒì¼ì„ S3ì—ì„œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        
        return matching_files
    

    def extract_smart_keywords_from_csv(self, company_name: str, start_date: str, end_date: str, top_keywords: int, use_ai_filter: bool = True) -> Dict:
        """
        CSV íŒŒì¼ì—ì„œ íŠ¹ì • ê¸°ì—…ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³  AI í•„í„°ë§ì„ ì ìš©í•©ë‹ˆë‹¤.
        
        Args:
            company_name: ê¸°ì—…ëª…
            start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            top_keywords: ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜
            use_ai_filter: AI í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€
        """
        # ê¸°ë³¸ í‚¤ì›Œë“œ ì¶”ì¶œ
        base_result = self.extract_keywords_from_csv(company_name, start_date, end_date, top_keywords * 2)  # ë” ë§ì€ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        if not use_ai_filter or not base_result.get('keywords'):
            return base_result
        
        try:
            logger.info(f"AI í•„í„°ë§ ì‹œì‘: {len(base_result['keywords'])}ê°œ í‚¤ì›Œë“œ")
            
            # SmartKeywordFilter ê°€ìš©ì„± í™•ì¸
            if not self.smart_filter.is_available():
                logger.warning("OpenAI APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. .env íŒŒì¼ì˜ OPENAI_API_KEYë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                base_result['ai_filtered'] = False
                base_result['ai_analysis'] = "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
                base_result['message'] += " (AI í•„í„°ë§ ì‚¬ìš© ë¶ˆê°€)"
                return base_result
            
            # AI í•„í„°ë§ ì ìš©
            filtered_keywords, filtered_top_keywords = self.smart_filter.filter_stock_related_keywords(
                base_result['keywords'], 
                company_name, 
                top_keywords
            )
            
            # í•„í„°ë§ ê²°ê³¼ ê²€ì¦
            if not filtered_keywords:
                logger.warning("AI í•„í„°ë§ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì›ë³¸ í‚¤ì›Œë“œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                # ì›ë³¸ í‚¤ì›Œë“œì˜ ìƒìœ„ í‚¤ì›Œë“œë§Œ ë°˜í™˜
                original_top = list(base_result['keywords'].items())[:top_keywords]
                base_result['keywords'] = dict(original_top)
                base_result['ai_filtered'] = False
                base_result['ai_analysis'] = "AI í•„í„°ë§ì—ì„œ ìœ íš¨í•œ í‚¤ì›Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                base_result['original_keyword_count'] = len(base_result['keywords'])
                base_result['filtered_keyword_count'] = 0
                base_result['message'] += " (AI í•„í„°ë§ ê²°ê³¼ ì—†ìŒ)"
                return base_result
            
            # í‚¤ì›Œë“œ ë¶„ì„ ì¶”ê°€
            analysis = ""
            if self.smart_filter.is_available() and filtered_keywords:
                try:
                    analysis = self.smart_filter.get_keyword_analysis(filtered_keywords, company_name)
                except Exception as e:
                    logger.warning(f"AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")
                    analysis = "í‚¤ì›Œë“œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ê²°ê³¼ ì—…ë°ì´íŠ¸
            result = base_result.copy()
            result['keywords'] = filtered_keywords
            result['ai_analysis'] = analysis
            result['ai_filtered'] = True
            result['original_keyword_count'] = len(base_result['keywords'])
            result['filtered_keyword_count'] = len(filtered_keywords)
            
            # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ëŠ” í•„í„°ë§ëœ í‚¤ì›Œë“œë¡œ ë‹¤ì‹œ ì¶”ì¶œ
            if 'top_news_articles' in base_result and filtered_top_keywords:
                # í•„í„°ë§ëœ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¬ì¶”ì¶œ
                result['top_news_articles'] = self.re_extract_news_articles_with_filtered_keywords(
                    base_result['top_news_articles'], filtered_top_keywords
                )
            if self.smart_filter.is_available():
                result['message'] = f"AI í•„í„°ë§ ì™„ë£Œ: {len(base_result['keywords'])}ê°œ â†’ {len(filtered_keywords)}ê°œ í‚¤ì›Œë“œ (ì£¼ê°€ ê´€ë ¨ì„± ê¸°ì¤€)"
            else:
                result['message'] = f"ê·œì¹™ ê¸°ë°˜ í•„í„°ë§ ì™„ë£Œ: {len(base_result['keywords'])}ê°œ â†’ {len(filtered_keywords)}ê°œ í‚¤ì›Œë“œ (ì£¼ê°€ ê´€ë ¨ì„± ê¸°ì¤€)"
            
            logger.info(f"AI í•„í„°ë§ ì„±ê³µ: {len(base_result['keywords'])}ê°œ â†’ {len(filtered_keywords)}ê°œ")
            return result
            
        except Exception as e:
            logger.error(f"AI í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # AI í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ ë°˜í™˜
            base_result['ai_filtered'] = False
            base_result['ai_analysis'] = "AI í•„í„°ë§ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            base_result['message'] += " (AI í•„í„°ë§ ì‹¤íŒ¨ë¡œ ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜)"
            return base_result

    def get_total_file_size(self, csv_files: List[str]) -> int:
        """S3ì—ì„œ íŒŒì¼ë“¤ì˜ ì´ í¬ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (ë°”ì´íŠ¸ ë‹¨ìœ„)"""
        total_size = 0
        try:
            for csv_path in csv_files:
                # s3a://bucket/path/file.csv -> bucket/path/file.csv
                s3_key = csv_path.replace(f"s3a://{self.s3_bucket}/", "")
                
                response = self.s3_client.head_object(
                    Bucket=self.s3_bucket,
                    Key=s3_key
                )
                file_size = response['ContentLength']
                total_size += file_size
                logger.info(f"íŒŒì¼ í¬ê¸°: {os.path.basename(csv_path)} - {file_size / (1024**3):.2f} GB")
                
        except Exception as e:
            logger.warning(f"íŒŒì¼ í¬ê¸° ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0
            
        return total_size

    def extract_keywords_from_csv(self, company_name: str, start_date: str, end_date: str, top_keywords: int) -> Dict:
        """
        CSV íŒŒì¼ì—ì„œ íŠ¹ì • ê¸°ì—…ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        íŒŒì¼ í¬ê¸°ì— ë”°ë¼ Spark ë˜ëŠ” Pandasë¥¼ ìë™ ì„ íƒí•©ë‹ˆë‹¤.
        """
        try:
            # CSV íŒŒì¼ë“¤ ê²½ë¡œ ì°¾ê¸°
            csv_files = self.find_csv_files(start_date, end_date)
            
            # íŒŒì¼ í¬ê¸° ê³„ì‚°
            total_size = self.get_total_file_size(csv_files)
            total_size_gb = total_size / (1024**3)
            
            logger.info(f"ì´ íŒŒì¼ í¬ê¸°: {total_size_gb:.2f} GB")
            
            # 15GB ì´ìƒì´ë©´ Spark ì‚¬ìš©
            if total_size_gb >= 10.0:
                logger.info("ğŸš€ ì—”ì§„ ì„ íƒ: PySpark (íŒŒì¼ í¬ê¸° 15GB ì´ìƒ)")
                # Spark ì´ˆê¸°í™” ì‹œë„
                try:
                    self.initialize_spark()
                    if self.spark_analyzer is None:
                        raise Exception("SparkAnalyzer ì´ˆê¸°í™” ì‹¤íŒ¨")
                    return self.spark_analyzer.extract_keywords_with_spark(company_name, start_date, end_date, top_keywords, csv_files)
                except Exception as e:
                    logger.warning(f"âš ï¸ PySpark ì‹¤í–‰ ì‹¤íŒ¨: {e}, Pandasë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                    return self.pandas_analyzer.extract_keywords_with_pandas(company_name, start_date, end_date, top_keywords, csv_files)
            else:
                logger.info("ğŸ¼ ì—”ì§„ ì„ íƒ: Pandas (íŒŒì¼ í¬ê¸° 15GB ë¯¸ë§Œ)")
                return self.pandas_analyzer.extract_keywords_with_pandas(company_name, start_date, end_date, top_keywords, csv_files)
                
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨ìœ¼ë¡œ pandas ì‚¬ìš©
            logger.info("âš ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ Pandas ì—”ì§„ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
            return self.pandas_analyzer.extract_keywords_with_pandas(company_name, start_date, end_date, top_keywords, csv_files)

    def re_extract_news_articles_with_filtered_keywords(self, original_articles, filtered_keywords):
        """
        AI í•„í„°ë§ëœ í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¬ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            original_articles: ì›ë³¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            filtered_keywords: AI í•„í„°ë§ëœ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[Dict]: í•„í„°ë§ëœ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” ë‰´ìŠ¤ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not original_articles or not filtered_keywords:
                return []
            
            # í•„í„°ë§ëœ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” ê¸°ì‚¬ë“¤ë§Œ ì¶”ì¶œ
            filtered_articles = []
            
            for article in original_articles:
                # ê¸°ì‚¬ì˜ í‚¤ì›Œë“œì™€ í•„í„°ë§ëœ í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
                matched_count = 0
                matched_keywords = []
                
                for filtered_keyword in filtered_keywords:
                    for article_keyword in article.get('all_keywords', []):
                        if filtered_keyword in article_keyword or article_keyword in filtered_keyword:
                            matched_count += 1
                            matched_keywords.append(filtered_keyword)
                            break  # ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€
                
                if matched_count > 0:
                    # ê¸°ì‚¬ ì •ë³´ ì—…ë°ì´íŠ¸
                    updated_article = article.copy()
                    updated_article['matched_keywords_count'] = matched_count
                    updated_article['matched_keywords'] = list(set(matched_keywords))
                    filtered_articles.append(updated_article)
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            filtered_articles.sort(key=lambda x: x['matched_keywords_count'], reverse=True)
            
            logger.info(f"AI í•„í„°ë§ëœ í‚¤ì›Œë“œë¡œ {len(filtered_articles)}ê°œ ë‰´ìŠ¤ ê¸°ì‚¬ ì¬ì¶”ì¶œ ì™„ë£Œ")
            
            return filtered_articles
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ê¸°ì‚¬ ì¬ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return original_articles  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def cleanup(self):
        """SparkSession ì •ë¦¬"""
        if self.spark:
            try:
                self.spark.stop()
                self.spark = None
                logger.info("SparkSessionì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                logger.warning(f"SparkSession ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
