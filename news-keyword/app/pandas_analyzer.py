"""
Pandasë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ì—”ì§„
ì¤‘ì†Œìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”
"""

import os
import re
from typing import Dict, List
from collections import Counter
import logging
import pandas as pd
import time
from csv_cache_manager import CSVCacheManager

logger = logging.getLogger(__name__)

class PandasAnalyzer:
    """Pandasë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë¶„ì„ê¸°"""
    
    def __init__(self):
        # CSV ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”
        self.csv_cache = CSVCacheManager()
    
    def extract_keywords_with_pandas(self, company_name: str, start_date: str, end_date: str, top_keywords: int, csv_files: List[str]) -> Dict:
        """
        pandasë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ (ë°±ì—… ë°©ë²•)
        ì—¬ëŸ¬ CSV íŒŒì¼ì„ ì½ì–´ì„œ í†µí•© ì²˜ë¦¬
        """
        logger.info("ğŸ¼ Pandas ì—”ì§„ìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        all_dataframes = []
        total_loaded_rows = 0
        
        # ëª¨ë“  CSV íŒŒì¼ ì½ê¸° (ìºì‹œ ìš°ì„  ì‚¬ìš©)
        for csv_path in csv_files:
            try:
                filename = os.path.basename(csv_path)
                logger.info(f"CSV íŒŒì¼ ì²˜ë¦¬ ì¤‘: {filename}")
                
                file_start_time = time.time()
                
                # 1. ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
                df = self.csv_cache.load_from_cache(csv_path)
                
                if df is not None:
                    # ìºì‹œì—ì„œ ë¡œë“œ ì„±ê³µ
                    file_read_time = time.time() - file_start_time
                    logger.info(f"ğŸš€ {filename} ìºì‹œ ë¡œë“œ: {len(df):,}í–‰, {file_read_time:.3f}ì´ˆ")
                else:
                    # 2. ìºì‹œì— ì—†ìœ¼ë©´ S3ì—ì„œ ì½ê³  ìºì‹œì— ì €ì¥
                    logger.info(f"ğŸ“¥ {filename} S3ì—ì„œ ì½ëŠ” ì¤‘...")
                    read_start_time = time.time()
                    df = pd.read_csv(csv_path, encoding='utf-8')
                    read_time = time.time() - read_start_time
                    
                    # ìºì‹œì— ì €ì¥
                    cache_saved = self.csv_cache.save_to_cache(csv_path, df)
                    
                    file_read_time = time.time() - file_start_time
                    cache_status = "âœ… ìºì‹œë¨" if cache_saved else "âŒ ìºì‹œ ì‹¤íŒ¨"
                    logger.info(f"ğŸ“ {filename} S3 ì½ê¸°: {len(df):,}í–‰, {read_time:.2f}ì´ˆ ({cache_status})")
                
                all_dataframes.append(df)
                total_loaded_rows += len(df)
                
            except Exception as e:
                logger.warning(f"CSV íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {csv_path}, ì˜¤ë¥˜: {e}")
                continue
        
        if not all_dataframes:
            raise FileNotFoundError("ì½ì„ ìˆ˜ ìˆëŠ” CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ ë³‘í•©
        df = pd.concat(all_dataframes, ignore_index=True)
        logger.info(f"ì´ {len(csv_files)}ê°œ íŒŒì¼ì—ì„œ {total_loaded_rows}ê°œ í–‰ ë¡œë“œ ì™„ë£Œ")
        logger.info(f"ë³‘í•© í›„ ì´ {len(df)}ê°œ í–‰")
        logger.info(f"ì»¬ëŸ¼ëª…: {list(df.columns)}")
        
        # ê¸°ì—… í•„í„°ë§ (ê¸°ê´€ ì»¬ëŸ¼ì—ì„œ í•´ë‹¹ ê¸°ì—…ì´ í¬í•¨ëœ í–‰ë“¤ì„ ê°€ì ¸ì˜´)
        if 'ê¸°ê´€' in df.columns:
            # ê¸°ê´€ ì»¬ëŸ¼ì— NaNì´ ì•„ë‹ˆê³  íšŒì‚¬ëª…ì´ í¬í•¨ëœ í–‰ í•„í„°ë§
            mask = df['ê¸°ê´€'].notna() & df['ê¸°ê´€'].str.contains(company_name, na=False, regex=False)
            company_filtered_df = df[mask]
            company_count = len(company_filtered_df)
            
            logger.info(f"'{company_name}' ê´€ë ¨ ë‰´ìŠ¤: {company_count}ê°œ (ê¸°ê´€ í•„í„°ë§ í›„)")
            
            # ë‚ ì§œ í•„í„°ë§ ì ìš©
            date_filtered_df = self.apply_date_filter(company_filtered_df, start_date, end_date)
            total_count = len(date_filtered_df)
            
            logger.info(f"ë‚ ì§œ í•„í„°ë§ í›„ ë‰´ìŠ¤: {total_count}ê°œ ({start_date}-{end_date})")
            
            # ìµœì¢… í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ ì‚¬ìš©
            filtered_df = date_filtered_df
            
            if total_count == 0:
                return {
                    "company_name": company_name,
                    "period": f"{start_date}-{end_date}",
                    "total_news_count": 0,
                    "daily_news_count": {},
                    "keywords": {},
                    "message": f"'{company_name}'ì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
            
            # ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
            daily_news_count = self.calculate_daily_news_count(filtered_df, start_date, end_date)
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê¸°ì¡´ í‚¤ì›Œë“œ ì»¬ëŸ¼ ì‚¬ìš©)
            if 'í‚¤ì›Œë“œ' in df.columns:
                # í‚¤ì›Œë“œ ì»¬ëŸ¼ì—ì„œ í‚¤ì›Œë“œ ë¶„ë¦¬ ë° ì •ë¦¬
                all_keywords = []
                for keywords_str in filtered_df['í‚¤ì›Œë“œ'].dropna():
                    keywords = [re.sub(r'[^ê°€-í£a-zA-Z0-9\s]', '', k.strip()) for k in keywords_str.split(',') if k.strip()]
                    all_keywords.extend([k for k in keywords if len(k) >= 2])
                
                # ê¸°ì—…ëª… ìì²´ëŠ” í‚¤ì›Œë“œì—ì„œ ì œì™¸
                all_keywords = [k for k in all_keywords if company_name not in k]
                
                # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
                keyword_counter = Counter(all_keywords)
                
                # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                keywords_dict = dict(keyword_counter.most_common())
                
                # ìƒìœ„ í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ ì¶”ì¶œ
                top_keywords_list = list(keywords_dict.keys())[:top_keywords]
                top_news_articles = self.extract_top_news_articles(filtered_df, top_keywords_list)
                
                # ìºì‹œ í†µê³„ ì¶œë ¥
                self.csv_cache.print_cache_stats()
                
                return {
                    "company_name": company_name,
                    "period": f"{start_date}-{end_date}",
                    "total_news_count": total_count,
                    "daily_news_count": daily_news_count,
                    "keywords": keywords_dict,
                    "top_news_articles": top_news_articles,
                    "message": f"ğŸ¼ Pandas ì—”ì§„ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤. ì´ {len(keywords_dict)}ê°œ í‚¤ì›Œë“œ ë°œê²¬ (íŒŒì¼ {len(csv_files)}ê°œ ì²˜ë¦¬)"
                }
            else:
                return {
                    "company_name": company_name,
                    "period": f"{start_date}-{end_date}",
                    "total_news_count": total_count,
                    "daily_news_count": daily_news_count,
                    "keywords": {},
                    "message": "í‚¤ì›Œë“œ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                }
        else:
            raise ValueError("ê¸°ê´€ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    def apply_date_filter(self, df, start_date: str, end_date: str) -> pd.DataFrame:
        """
        ë‚ ì§œ ì»¬ëŸ¼ì„ ì‚¬ìš©í•˜ì—¬ ì„¤ì •ëœ ê¸°ê°„ ë‚´ì˜ ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            df: í•„í„°ë§í•  ë°ì´í„°í”„ë ˆì„
            start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            
        Returns:
            pd.DataFrame: ë‚ ì§œ í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
        """
        try:
            from datetime import datetime
            
            # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            date_column = None
            for col in ['ì¼ì', 'ë‚ ì§œ', 'date', 'Date', 'DATE']:
                if col in df.columns:
                    date_column = col
                    break
            
            if date_column is None:
                logger.warning("ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                return df
            
            logger.info(f"ë‚ ì§œ í•„í„°ë§ì— ì‚¬ìš©í•  ì»¬ëŸ¼: {date_column}")
            
            # ë‚ ì§œ ë²”ìœ„ë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
            start_dt = datetime.strptime(start_date, "%Y%m%d")
            end_dt = datetime.strptime(end_date, "%Y%m%d")
            
            # ë‚ ì§œ ì»¬ëŸ¼ì˜ ê°’ë“¤ì„ íŒŒì‹±í•˜ì—¬ í•„í„°ë§
            def parse_date(date_str):
                """ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ì„ íŒŒì‹±í•©ë‹ˆë‹¤."""
                if pd.isna(date_str):
                    return None
                
                date_str = str(date_str).strip()
                
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì‹œë„
                date_formats = [
                    "%Y%m%d",      # 20210811
                    "%Y-%m-%d",   # 2021-08-11
                    "%Y/%m/%d",   # 2021/08/11
                    "%Y.%m.%d",   # 2021.08.11
                    "%Y-%m-%d %H:%M:%S",  # 2021-08-11 10:30:00
                    "%Y/%m/%d %H:%M:%S",  # 2021/08/11 10:30:00
                ]
                
                for fmt in date_formats:
                    try:
                        return datetime.strptime(date_str, fmt)
                    except ValueError:
                        continue
                
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ None ë°˜í™˜
                return None
            
            # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
            df_copy = df.copy()
            df_copy['parsed_date'] = df_copy[date_column].apply(parse_date)
            
            # ìœ íš¨í•œ ë‚ ì§œë§Œ í•„í„°ë§
            valid_dates_mask = df_copy['parsed_date'].notna()
            df_with_dates = df_copy[valid_dates_mask]
            
            logger.info(f"ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë‰´ìŠ¤: {len(df_with_dates)}ê°œ")
            
            if len(df_with_dates) == 0:
                logger.warning("ìœ íš¨í•œ ë‚ ì§œê°€ ìˆëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return df.iloc[0:0]  # ë¹ˆ ë°ì´í„°í”„ë ˆì„ ë°˜í™˜
            
            # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
            date_range_mask = (
                (df_with_dates['parsed_date'] >= start_dt) & 
                (df_with_dates['parsed_date'] <= end_dt)
            )
            
            filtered_df = df_with_dates[date_range_mask]
            
            # ì›ë³¸ ì»¬ëŸ¼ë§Œ ìœ ì§€ (parsed_date ì»¬ëŸ¼ ì œê±°)
            result_df = filtered_df.drop('parsed_date', axis=1)
            
            logger.info(f"ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ ì™„ë£Œ: {len(result_df)}ê°œ ë‰´ìŠ¤")
            
            # ìƒ˜í”Œ ë‚ ì§œ ê°’ ë¡œê·¸ ì¶œë ¥
            if len(result_df) > 0:
                sample_dates = result_df[date_column].head(3).tolist()
                logger.info(f"í•„í„°ë§ëœ ìƒ˜í”Œ ë‚ ì§œ: {sample_dates}")
            
            return result_df
            
        except Exception as e:
            logger.error(f"ë‚ ì§œ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            logger.info("ë‚ ì§œ í•„í„°ë§ì„ ê±´ë„ˆë›°ê³  ì›ë³¸ ë°ì´í„°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return df
    
    def calculate_daily_news_count(self, filtered_df, start_date: str, end_date: str) -> Dict[str, int]:
        """
        ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Args:
            filtered_df: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
            start_date: ì‹œì‘ ë‚ ì§œ (YYYYMMDD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD)
            
        Returns:
            Dict[str, int]: ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ {"20210811": 15, "20210812": 23, ...}
        """
        try:
            from datetime import datetime, timedelta
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼ í™•ì¸
            logger.info(f"ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(filtered_df.columns)}")
            
            # ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ ì°¾ê¸°
            date_column = None
            for col in ['ì¼ì', 'ë‚ ì§œ', 'date', 'Date', 'DATE']:
                if col in filtered_df.columns:
                    date_column = col
                    break
            
            if date_column is None:
                logger.warning("ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return {}
            
            logger.info(f"ë‚ ì§œ ì»¬ëŸ¼ ì‚¬ìš©: {date_column}")
            
            # ë‚ ì§œ ë²”ìœ„ ìƒì„±
            start_dt = datetime.strptime(start_date, "%Y%m%d")
            end_dt = datetime.strptime(end_date, "%Y%m%d")
            
            daily_count = {}
            current_date = start_dt
            
            # ê° ë‚ ì§œë³„ë¡œ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
            while current_date <= end_dt:
                date_str = current_date.strftime("%Y%m%d")
                
                # í•´ë‹¹ ë‚ ì§œì˜ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚°
                try:
                    # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
                    date_patterns = [
                        date_str,  # 20210811
                        f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}",  # 2021-08-11
                        f"{date_str[:4]}/{date_str[4:6]}/{date_str[6:8]}",  # 2021/08/11
                        f"{date_str[:4]}.{date_str[4:6]}.{date_str[6:8]}",  # 2021.08.11
                    ]
                    
                    # ê° íŒ¨í„´ì— ëŒ€í•´ ë§¤ì¹­ ì‹œë„
                    count = 0
                    for pattern in date_patterns:
                        date_mask = filtered_df[date_column].astype(str).str.contains(pattern, na=False)
                        pattern_count = date_mask.sum()
                        if pattern_count > 0:
                            count = pattern_count
                            logger.debug(f"{date_str} ({pattern}): {count}ê°œ ë‰´ìŠ¤ ë°œê²¬")
                            break
                    
                    # ë””ë²„ê¹…ì„ ìœ„í•œ ë¡œê·¸
                    if count == 0:
                        # ìƒ˜í”Œ ë‚ ì§œ ê°’ í™•ì¸
                        sample_dates = filtered_df[date_column].dropna().head(3).tolist()
                        logger.debug(f"{date_str} ë§¤ì¹­ ì‹¤íŒ¨. ìƒ˜í”Œ ë‚ ì§œ ê°’: {sample_dates}")
                    
                except Exception as e:
                    logger.warning(f"ë‚ ì§œ {date_str} ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
                    count = 0
                
                daily_count[date_str] = int(count)
                current_date += timedelta(days=1)
            
            # ì´í•© ê²€ì¦
            total_daily_count = sum(daily_count.values())
            logger.info(f"ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚° ì™„ë£Œ: {len(daily_count)}ì¼, ì´í•©: {total_daily_count}ê°œ")
            logger.info(f"daily_news_count í•©ê³„: {total_daily_count}, total_news_count: {len(filtered_df)}")
            
            return daily_count
            
        except Exception as e:
            logger.warning(f"ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ê³„ì‚° ì¤‘ ì˜¤ë¥˜: {e}")
            return {}
    
    def extract_top_news_articles(self, filtered_df, top_keywords_list, max_articles=10):
        """
        ìƒìœ„ í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            filtered_df: í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„
            top_keywords_list: ìƒìœ„ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
            max_articles: ìµœëŒ€ ê¸°ì‚¬ ìˆ˜
            
        Returns:
            List[Dict]: ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ë¦¬ìŠ¤íŠ¸
        """
        try:
            if not top_keywords_list:
                return []
            
            # ê° ê¸°ì‚¬ì—ì„œ ìƒìœ„ í‚¤ì›Œë“œ ë§¤ì¹­ ê°œìˆ˜ ê³„ì‚°
            articles_with_score = []
            
            for idx, row in filtered_df.iterrows():
                article_keywords = []
                if pd.notna(row.get('í‚¤ì›Œë“œ', '')):
                    # ê¸°ì‚¬ í‚¤ì›Œë“œ ë¶„ë¦¬
                    article_keywords = [k.strip() for k in str(row['í‚¤ì›Œë“œ']).split(',') if k.strip()]
                
                # ìƒìœ„ í‚¤ì›Œë“œì™€ ë§¤ì¹­ë˜ëŠ” ê°œìˆ˜ ê³„ì‚°
                matched_count = 0
                matched_keywords = []
                for keyword in top_keywords_list:
                    for article_keyword in article_keywords:
                        if keyword in article_keyword or article_keyword in keyword:
                            matched_count += 1
                            matched_keywords.append(keyword)
                            break  # ì¤‘ë³µ ì¹´ìš´íŠ¸ ë°©ì§€
                
                if matched_count > 0:
                    # nan ê°’ ì²˜ë¦¬
                    title = row.get('ì œëª©', 'ì œëª© ì—†ìŒ')
                    if pd.isna(title):
                        title = 'ì œëª© ì—†ìŒ'
                    
                    date = row.get('ì¼ì', 'ì¼ì ì—†ìŒ')
                    if pd.isna(date):
                        date = 'ì¼ì ì—†ìŒ'
                    
                    url = row.get('URL', 'URL ì—†ìŒ')
                    if pd.isna(url):
                        url = 'URL ì—†ìŒ'
                    
                    articles_with_score.append({
                        'title': str(title),
                        'date': str(date),
                        'url': str(url),
                        'matched_keywords_count': matched_count,
                        'matched_keywords': list(set(matched_keywords)),
                        'all_keywords': article_keywords
                    })
            
            # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ê°œìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
            articles_with_score.sort(key=lambda x: x['matched_keywords_count'], reverse=True)
            
            # ìƒìœ„ ê¸°ì‚¬ë“¤ë§Œ ë°˜í™˜
            top_articles = articles_with_score[:max_articles]
            
            logger.info(f"ìƒìœ„ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ {len(top_articles)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
            
            return top_articles
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ê¸°ì‚¬ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return []
