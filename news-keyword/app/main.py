#!/usr/bin/env python3
"""
ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜
ê¸°ê°„ë³„ ê¸°ì—… í‚¤ì›Œë“œ ì¶”ì¶œ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from datetime import datetime
import logging
import os
import time
from typing import Optional, Dict, List
from contextlib import asynccontextmanager
from keyword_extractor import KeywordExtractor
from cache_manager import CacheManager
import glob
import math

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class KeywordRequest(BaseModel):
    """í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ ëª¨ë¸"""
    company_name: str  # ê¸°ì—…ëª… (ì˜ˆ: "ì‚¼ì„±ì „ì")
    start_date: str    # ì‹œì‘ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ì˜ˆ: "20200901")
    end_date: str      # ì¢…ë£Œ ë‚ ì§œ (YYYYMMDD í˜•ì‹, ì˜ˆ: "20200903")
    top_keywords: Optional[int] = 20  # ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜ (ê¸°ë³¸ê°’: 20)
    use_ai_filter: Optional[bool] = True  # AI í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)

class NewsArticle(BaseModel):
    """ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ëª¨ë¸"""
    title: str
    date: str
    url: str
    matched_keywords_count: int
    matched_keywords: List[str]

class KeywordResponse(BaseModel):
    """í‚¤ì›Œë“œ ì¶”ì¶œ ì‘ë‹µ ëª¨ë¸"""
    company_name: str
    period: str
    total_news_count: int
    daily_news_count: Optional[Dict[str, int]] = {}  # ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ {"20210811": 15, "20210812": 23, ...}
    keywords: Dict[str, int]  # {"í‚¤ì›Œë“œ": ë¹ˆë„ìˆ˜, ...}
    top_news_articles: Optional[List[NewsArticle]] = []  # ìƒìœ„ í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤
    message: str
    ai_filtered: Optional[bool] = False  # AI í•„í„°ë§ ì ìš© ì—¬ë¶€
    ai_analysis: Optional[str] = ""  # AI ë¶„ì„ ê²°ê³¼
    original_keyword_count: Optional[int] = 0  # ì›ë³¸ í‚¤ì›Œë“œ ê°œìˆ˜
    filtered_keyword_count: Optional[int] = 0  # í•„í„°ë§ëœ í‚¤ì›Œë“œ ê°œìˆ˜

# í‚¤ì›Œë“œ ì¶”ì¶œê¸° ë° ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
keyword_extractor = KeywordExtractor()
cache_manager = CacheManager()

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì•± ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    # ì‹œì‘ ì‹œ
    logger.info("FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
    yield
    # ì¢…ë£Œ ì‹œ
    keyword_extractor.cleanup()
    cache_manager.cleanup()
    logger.info("FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

app = FastAPI(
    title="ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ API",
    description="ê¸°ê°„ë³„ ê¸°ì—…ì˜ í‚¤ì›Œë“œ ì¶”ì¶œì„ ìœ„í•œ FastAPI ì„œë¹„ìŠ¤",
    version="1.0.0",
    lifespan=lifespan
)

@app.get("/")
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸"""
    return {
        "message": "ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ APIì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤!",
        "version": "1.0.0",
        "endpoints": {
            "í‚¤ì›Œë“œ ì¶”ì¶œ (AI í•„í„°ë§ í¬í•¨)": "/extract-keywords/ticker",
            "ìºì‹œ í†µê³„": "/cache/stats",
            "ìºì‹œ ì‚­ì œ": "/cache/clear",
            "API ë¬¸ì„œ": "/docs",
            "í—¬ìŠ¤ì²´í¬": "/health"
        },
        "features": {
            "ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ": "ê¸°ì¡´ í‚¤ì›Œë“œ ì¶”ì¶œ ë°©ì‹",
            "AI ìŠ¤ë§ˆíŠ¸ í•„í„°ë§": "OpenAIë¥¼ í™œìš©í•œ ì£¼ê°€ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§",
            "í‚¤ì›Œë“œ ë¶„ì„": "AI ê¸°ë°˜ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„",
            "SQLite ìºì‹±": "ë™ì¼í•œ ìš”ì²­ì— ëŒ€í•œ ë¹ ë¥¸ ì‘ë‹µ ì œê³µ",
            "ìë™ ìºì‹œ ê´€ë¦¬": "ì˜¤ë˜ëœ ìºì‹œ ìë™ ì‚­ì œ ë° í†µê³„ ì œê³µ"
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

@app.get("/cache/stats")
async def get_cache_stats():
    """ìºì‹œ í†µê³„ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        stats = cache_manager.get_cache_stats()
        return {
            "status": "success",
            "cache_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ìºì‹œ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìºì‹œ í†µê³„ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.delete("/cache/clear")
async def clear_old_cache(days: int = 30):
    """ì˜¤ë˜ëœ ìºì‹œ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        deleted_count = cache_manager.clear_old_cache(days)
        return {
            "status": "success",
            "message": f"{deleted_count}ê°œì˜ ì˜¤ë˜ëœ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_count": deleted_count,
            "days_threshold": days,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@app.post("/extract-keywords/ticker", response_model=KeywordResponse)
async def extract_keywords(request: KeywordRequest):
    """
    ê¸°ì—…ì˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸ (AI í•„í„°ë§ ì§€ì›)
    
    Args:
        request: í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­ (íšŒì‚¬ëª…, ì‹œì‘ì¼ì, ì¢…ë£Œì¼ì, ìƒìœ„ í‚¤ì›Œë“œ ê°œìˆ˜, AI í•„í„°ë§ ì‚¬ìš© ì—¬ë¶€)
    
    Returns:
        KeywordResponse: ì¶”ì¶œëœ í‚¤ì›Œë“œì™€ ë¹ˆë„ìˆ˜, AI ë¶„ì„ ê²°ê³¼
    
    Features:
        - ë¹ˆë„ìˆ˜ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        - OpenAIë¥¼ í™œìš©í•œ ì£¼ê°€ ê´€ë ¨ í‚¤ì›Œë“œ í•„í„°ë§
        - AI ê¸°ë°˜ í‚¤ì›Œë“œ íŠ¸ë Œë“œ ë¶„ì„
    
    Example:
        POST /extract-keywords
        {
            "company_name": "ì‚¼ì„±ì „ì",
            "start_date": "20200901", 
            "end_date": "20200903",
            "top_keywords": 20,
            "use_ai_filter": true
        }
    """
    start_time = time.time()
    
    try:
        logger.info(f"ğŸš€ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­: {request.company_name}, {request.start_date}-{request.end_date}")
        
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        try:
            datetime.strptime(request.start_date, "%Y%m%d")
            datetime.strptime(request.end_date, "%Y%m%d")
        except ValueError:
            raise HTTPException(status_code=400, detail="ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. YYYYMMDD í˜•ì‹ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.")
        
        # ìºì‹œì—ì„œ ê²°ê³¼ ì¡°íšŒ
        cached_result = cache_manager.get_cached_result(
            company_name=request.company_name,
            start_date=request.start_date,
            end_date=request.end_date,
            top_keywords=request.top_keywords,
            use_ai_filter=request.use_ai_filter
        )
        
        if cached_result:
            logger.info(f"ğŸ¯ ìºì‹œì—ì„œ ê²°ê³¼ ë°˜í™˜: {request.company_name}")
            result = cached_result
        else:
            logger.info(f"ğŸ” ìºì‹œ ë¯¸ìŠ¤ - í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰: {request.company_name}")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤í–‰ (AI í•„í„°ë§ ì˜µì…˜ í¬í•¨)
            if request.use_ai_filter:
                result = keyword_extractor.extract_smart_keywords_from_csv(
                    company_name=request.company_name,
                    start_date=request.start_date,
                    end_date=request.end_date,
                    top_keywords=request.top_keywords,
                    use_ai_filter=request.use_ai_filter
                )
            else:
                result = keyword_extractor.extract_keywords_from_csv(
                    company_name=request.company_name,
                    start_date=request.start_date,
                    end_date=request.end_date,
                    top_keywords=request.top_keywords
                )
            
            # ê²°ê³¼ë¥¼ ìºì‹œì— ì €ì¥
            cache_saved = cache_manager.save_result(
                company_name=request.company_name,
                start_date=request.start_date,
                end_date=request.end_date,
                top_keywords=request.top_keywords,
                use_ai_filter=request.use_ai_filter,
                result_data=result
            )
            
            if cache_saved:
                logger.info(f"ğŸ’¾ ê²°ê³¼ ìºì‹œ ì €ì¥ ì™„ë£Œ: {request.company_name}")
            else:
                logger.info(f"âš ï¸ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ ë˜ëŠ” ì´ë¯¸ ì¡´ì¬: {request.company_name}")
        
        # ì‘ë‹µ í˜•ì‹ì— ë§ê²Œ ë³€í™˜ (ìƒìœ„ í‚¤ì›Œë“œë§Œ)
        top_keywords_dict = dict(list(result["keywords"].items())[:request.top_keywords])
        
        # ë‰´ìŠ¤ ê¸°ì‚¬ ì •ë³´ ë³€í™˜
        top_news_articles = []
        if "top_news_articles" in result and result["top_news_articles"]:
            for article in result["top_news_articles"]:
                # nan ê°’ ì²˜ë¦¬
                url = article.get("url", "URL ì—†ìŒ")
                if url is None or (isinstance(url, float) and str(url).lower() == 'nan'):
                    url = "URL ì—†ìŒ"
                
                title = article.get("title", "ì œëª© ì—†ìŒ")
                if title is None or (isinstance(title, float) and str(title).lower() == 'nan'):
                    title = "ì œëª© ì—†ìŒ"
                
                date = article.get("date", "ë‚ ì§œ ì—†ìŒ")
                if date is None or (isinstance(date, float) and str(date).lower() == 'nan'):
                    date = "ë‚ ì§œ ì—†ìŒ"
                
                top_news_articles.append(NewsArticle(
                    title=str(title),
                    date=str(date),
                    url=str(url),
                    matched_keywords_count=article.get("matched_keywords_count", 0),
                    matched_keywords=article.get("matched_keywords", [])
                ))
        
        response = KeywordResponse(
            company_name=result["company_name"],
            period=result["period"],
            total_news_count=result["total_news_count"],
            daily_news_count=result.get("daily_news_count", {}),
            keywords=top_keywords_dict,
            top_news_articles=top_news_articles,
            message=result["message"],
            ai_filtered=result.get("ai_filtered", False),
            ai_analysis=result.get("ai_analysis", ""),
            original_keyword_count=result.get("original_keyword_count", 0),
            filtered_keyword_count=result.get("filtered_keyword_count", 0)
        )
        
        # ì´ ì†Œìš” ì‹œê°„ ê³„ì‚°
        total_time = time.time() - start_time
        
        # ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜ ë¡œê·¸ ì¶œë ¥
        daily_count = result.get("daily_news_count", {})
        if daily_count:
            daily_summary = ", ".join([f"{date}: {count}ê°œ" for date, count in daily_count.items()])
            logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: '{request.company_name}' ê´€ë ¨ ë‰´ìŠ¤ {result['total_news_count']}ê°œì—ì„œ {len(result['keywords'])}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
            logger.info(f"ë‚ ì§œë³„ ë‰´ìŠ¤ ê°œìˆ˜: {daily_summary}")
        else:
            logger.info(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: '{request.company_name}' ê´€ë ¨ ë‰´ìŠ¤ {result['total_news_count']}ê°œì—ì„œ {len(result['keywords'])}ê°œ í‚¤ì›Œë“œ ì¶”ì¶œ")
        
        # ì´ API ì‘ë‹µ ì‹œê°„ ì¶œë ¥
        logger.info(f"ğŸ¯ ì´ API ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ")
        
        return response
        
    except FileNotFoundError as e:
        total_time = time.time() - start_time
        logger.error(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
        logger.error(f"âŒ API ì‹¤íŒ¨ ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ")
        raise HTTPException(status_code=404, detail=str(e))
    except ValueError as e:
        total_time = time.time() - start_time
        logger.error(f"ì˜ëª»ëœ ìš”ì²­: {str(e)}")
        logger.error(f"âŒ API ì‹¤íŒ¨ ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ")
        raise HTTPException(status_code=400, detail=str(e))
    except Exception as e:
        total_time = time.time() - start_time
        logger.error(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {str(e)}")
        logger.error(f"âŒ API ì‹¤íŒ¨ ì‘ë‹µ ì‹œê°„: {total_time:.2f}ì´ˆ")
        raise HTTPException(status_code=500, detail=f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


# ------------------------------
# ê¸°ì—… ì˜í–¥ë ¥ API (Parquet â†’ pyarrow)
# ------------------------------

class InfluenceItem(BaseModel):
    rank: int
    company: str
    score: float
    relative: float
    score_type: str  # "pagerank" | "degree"


def _resolve_parquet_glob(base_path: str) -> str:
    # S3 ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©. ë””ë ‰í„°ë¦¬ë¡œ ëë‚˜ë©´ *.parquet ìë™ ë¶€ì—¬
    if isinstance(base_path, str) and base_path.lower().startswith("s3://"):
        if base_path.endswith("/"):
            return base_path + "*.parquet"
        return base_path
    if os.path.isdir(base_path):
        return os.path.join(base_path, "*.parquet")
    return base_path


def _load_influence_with_pyarrow(path_glob: str, top_n: int, target_company: Optional[str] = None):
    try:
        import pyarrow.parquet as pq
        import pyarrow as pa
        import pandas as pd
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"pyarrow/pandas ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ë¡œì»¬ vs S3 êµ¬ë¶„ ë° íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
    file_list: List[str] = []
    is_s3 = isinstance(path_glob, str) and path_glob.lower().startswith("s3://")
    has_wildcard = isinstance(path_glob, str) and ("*" in path_glob or "?" in path_glob or "[" in path_glob)

    if is_s3:
        try:
            import fsspec
            fs = fsspec.filesystem("s3")
            if has_wildcard:
                file_list = fs.glob(path_glob)
            else:
                file_list = [path_glob]
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"S3 ì ‘ê·¼ ì‹¤íŒ¨: {e}")
    else:
        file_list = glob.glob(path_glob)

    if not file_list:
        raise HTTPException(status_code=404, detail=f"Parquet íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path_glob}")

    try:
        if is_s3:
            s3fs = pa.fs.S3FileSystem()
            tables = [pq.read_table(fp, filesystem=s3fs) for fp in file_list]
        else:
            tables = [pq.read_table(fp) for fp in file_list]
        table = pa.concat_tables(tables, promote=True)
        pdf = table.to_pandas()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")

    cols = set(pdf.columns)

    # Case 1: PageRank ê²°ê³¼
    if {"company", "pagerank_score"}.issubset(cols):
        df = pdf[["company", "pagerank_score"]].copy()
        if target_company:
            # ë¶€ë¶„ ì¼ì¹˜ë¡œ í•„í„°ë§
            df = df[df["company"].astype(str).str.contains(target_company)]
        if df.empty:
            return [], "pagerank"
        df = df.sort_values("pagerank_score", ascending=False)
        max_score = float(df.iloc[0]["pagerank_score"]) if not df.empty else 0.0
        top_df = df.head(top_n)
        items = []
        for idx, row in enumerate(top_df.itertuples(index=False), 1):
            score = float(getattr(row, "pagerank_score") or 0.0)
            rel = (score / max_score * 100.0) if max_score > 0 else 0.0
            items.append({
                "rank": idx,
                "company": str(getattr(row, "company")),
                "score": round(score, 10),
                "relative": round(rel, 2),
                "score_type": "pagerank"
            })
        return items, "pagerank"

    # Case 2: ì—°ê²° ê·¸ë˜í”„ ê²°ê³¼ â†’ ì„ì‹œ ì˜í–¥ë ¥ (ê°€ì¤‘ in/out-degree í•©)
    if {"src", "dst", "weight"}.issubset(cols):
        df = pdf[["src", "dst", "weight"]].copy()
        if target_company:
            mask = df["src"].astype(str).str.contains(target_company) | df["dst"].astype(str).str.contains(target_company)
            df = df[mask]
        if df.empty:
            return [], "degree"
        out_w = df.groupby("src")["weight"].sum()
        in_w = df.groupby("dst")["weight"].sum()
        companies = sorted(set(out_w.index).union(in_w.index))
        influence = []
        for c in companies:
            influence.append((c, float(out_w.get(c, 0.0)) + float(in_w.get(c, 0.0))))
        influence.sort(key=lambda x: x[1], reverse=True)
        topk = influence[: top_n]
        if not topk:
            return [], "degree"
        max_score = topk[0][1]
        items = []
        for idx, (company, score) in enumerate(topk, 1):
            rel = (score / max_score * 100.0) if max_score > 0 else 0.0
            items.append({
                "rank": idx,
                "company": str(company),
                "score": float(round(score, 6)),
                "relative": float(round(rel, 2)),
                "score_type": "degree"
            })
        return items, "degree"

    # Unknown schema
    raise HTTPException(status_code=422, detail="ì§€ì›í•˜ì§€ ì•ŠëŠ” Parquet ìŠ¤í‚¤ë§ˆì…ë‹ˆë‹¤. 'company,pagerank_score' ë˜ëŠ” 'src,dst,weight'ë¥¼ ê¸°ëŒ€í•©ë‹ˆë‹¤.")


@app.get("/influence", response_model=List[InfluenceItem])
async def get_influence(path: str = "s3://cheesecrust-spark-data-bucket/outputs/pagerank/pagerank/", top: int = 20, company: Optional[str] = None):
    """
    Parquet ê²°ê³¼ì—ì„œ ê¸°ì—… ì˜í–¥ë ¥ ìˆœìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ê¸°ë³¸ ê²½ë¡œ: /output
    - ê¸°ë³¸ top: 20
    - company ì§€ì • ì‹œ í•´ë‹¹ ì´ë¦„ì´ í¬í•¨ëœ ê¸°ì—…ë§Œ í•„í„°ë§í•˜ì—¬ ìˆœìœ„ ë°˜í™˜
    """
    if top <= 0:
        raise HTTPException(status_code=400, detail="top ì€ 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    path_glob = _resolve_parquet_glob(path)
    items, score_type = _load_influence_with_pyarrow(path_glob, top, company)
    return items

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ë‰´ìŠ¤ í‚¤ì›Œë“œ ì¶”ì¶œ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ğŸ“Š API ë¬¸ì„œ: http://localhost:8888/docs")
    print("ğŸ’“ í—¬ìŠ¤ì²´í¬: http://localhost:8888/health") 
    print("ğŸ¤– AI ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì§€ì›")
    print("â¹ï¸  Ctrl+Cë¥¼ ëˆŒëŸ¬ ì„œë²„ë¥¼ ì¢…ë£Œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    uvicorn.run(
        app, 
        host="0.0.0.0", 
        port=8888,
        log_level="info"
    )